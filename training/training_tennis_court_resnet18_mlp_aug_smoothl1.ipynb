{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm41rvPhxRHa",
        "outputId": "bbe32078-1d11-4487-e0dc-a66d343eadb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: apri google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkDOC4Z_xyw-",
        "outputId": "00e4cffb-3d26-4be6-fffd-44a90145c380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TennisAnalysis2/training\n"
          ]
        }
      ],
      "source": [
        "# prompt: cambia directory a /content/drive/MyDrive/TennisAnalytics/training\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/TennisAnalysis2/training')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E7wF8JSIyeUs"
      },
      "outputs": [],
      "source": [
        "# First, install the ultralytics library\n",
        "#!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXXMhfyCwZdg"
      },
      "source": [
        "# Fine tuning Yolo for ball detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xhB9h40x-J3",
        "outputId": "c8654063-d57b-4c36-d227-9e481016ee6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics 8.3.162 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=tennis-ball-detection-6/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov5l6u.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 81.8MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      7040  ultralytics.nn.modules.conv.Conv             [3, 64, 6, 2, 2]              \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   1118208  ultralytics.nn.modules.block.C3              [256, 256, 6]                 \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  9   6433792  ultralytics.nn.modules.block.C3              [512, 512, 9]                 \n",
            "  7                  -1  1   3540480  ultralytics.nn.modules.conv.Conv             [512, 768, 3, 2]              \n",
            "  8                  -1  3   5611008  ultralytics.nn.modules.block.C3              [768, 768, 3]                 \n",
            "  9                  -1  1   7079936  ultralytics.nn.modules.conv.Conv             [768, 1024, 3, 2]             \n",
            " 10                  -1  3   9971712  ultralytics.nn.modules.block.C3              [1024, 1024, 3]               \n",
            " 11                  -1  1   2624512  ultralytics.nn.modules.block.SPPF            [1024, 1024, 5]               \n",
            " 12                  -1  1    787968  ultralytics.nn.modules.conv.Conv             [1024, 768, 1, 1]             \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   6200832  ultralytics.nn.modules.block.C3              [1536, 768, 3, False]         \n",
            " 16                  -1  1    394240  ultralytics.nn.modules.conv.Conv             [768, 512, 1, 1]              \n",
            " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 18             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  3   2757632  ultralytics.nn.modules.block.C3              [1024, 512, 3, False]         \n",
            " 20                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
            " 21                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 22             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 23                  -1  3    690688  ultralytics.nn.modules.block.C3              [512, 256, 3, False]          \n",
            " 24                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 25            [-1, 20]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 26                  -1  3   2495488  ultralytics.nn.modules.block.C3              [512, 512, 3, False]          \n",
            " 27                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 28            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 29                  -1  3   5807616  ultralytics.nn.modules.block.C3              [1024, 768, 3, False]         \n",
            " 30                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            " 31            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 32                  -1  3  10496000  ultralytics.nn.modules.block.C3              [1536, 1024, 3, False]        \n",
            " 33    [23, 26, 29, 32]  1   9902356  ultralytics.nn.modules.head.Detect           [1, [256, 512, 768, 1024]]    \n",
            "YOLOv5l6u summary: 314 layers, 86,018,708 parameters, 86,018,692 gradients, 137.7 GFLOPs\n",
            "\n",
            "Transferred 891/899 items from pretrained weights\n",
            "Freezing layer 'model.33.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 192MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.7Â±0.3 ms, read: 0.1Â±0.0 MB/s, size: 91.9 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/TennisAnalytics/training/tennis-ball-detection-6/tennis-ball-detection-6/train/labels... 428 images, 2 backgrounds, 0 corrupt: 100% 428/428 [04:34<00:00,  1.56it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/TennisAnalytics/training/tennis-ball-detection-6/tennis-ball-detection-6/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.7Â±0.3 ms, read: 0.1Â±0.1 MB/s, size: 83.0 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/TennisAnalytics/training/tennis-ball-detection-6/tennis-ball-detection-6/valid/labels... 100 images, 0 backgrounds, 0 corrupt: 100% 100/100 [01:03<00:00,  1.58it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/TennisAnalytics/training/tennis-ball-detection-6/tennis-ball-detection-6/valid/labels.cache\n",
            "Plotting labels to runs/detect/train7/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 147 weight(decay=0.0), 156 weight(decay=0.0005), 155 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100        10G      3.805      136.5       1.03         19        640: 100% 27/27 [00:24<00:00,  1.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.39it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100      11.2G       4.25      18.94      1.148         20        640: 100% 27/27 [00:23<00:00,  1.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  3.00it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100      10.8G      3.911      3.922      1.051         20        640: 100% 27/27 [00:30<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.89it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100      10.8G      3.454      2.897     0.9923         18        640: 100% 27/27 [00:31<00:00,  1.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.88it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100      10.9G      3.719        2.8     0.9969         14        640: 100% 27/27 [00:30<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.82it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100      10.8G      3.505      2.705     0.9492         21        640: 100% 27/27 [00:30<00:00,  1.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.23it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100      10.8G      3.377      2.277     0.9496         21        640: 100% 27/27 [00:20<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.33it/s]\n",
            "                   all        100        101      0.383     0.0198     0.0104    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100      11.1G      3.364      2.579     0.9571         19        640: 100% 27/27 [00:21<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.59it/s]\n",
            "                   all        100        101          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100      10.9G      3.174      2.235     0.9319         16        640: 100% 27/27 [00:20<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.24it/s]\n",
            "                   all        100        101       0.55      0.337      0.333     0.0808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100      11.1G      3.176      2.023     0.9483         20        640: 100% 27/27 [00:30<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.02it/s]\n",
            "                   all        100        101       0.38      0.198      0.184     0.0439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100      11.1G      3.183      2.086     0.9075         20        640: 100% 27/27 [00:21<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.43it/s]\n",
            "                   all        100        101      0.698      0.347       0.38      0.105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100      10.8G      3.183      2.082     0.9333         30        640: 100% 27/27 [00:21<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.91it/s]\n",
            "                   all        100        101      0.587      0.183      0.229     0.0616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100      10.8G      3.004      1.817     0.9051         18        640: 100% 27/27 [00:20<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.85it/s]\n",
            "                   all        100        101      0.661      0.193      0.294     0.0834\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100      10.7G      2.926      1.894     0.8927         13        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.49it/s]\n",
            "                   all        100        101       0.14     0.0792      0.103     0.0397\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100      10.8G      2.919       1.92     0.8994         25        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.95it/s]\n",
            "                   all        100        101      0.566      0.267      0.279      0.101\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100        11G      3.108      2.007     0.8794         24        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.80it/s]\n",
            "                   all        100        101      0.482      0.228      0.169     0.0382\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100      10.8G      2.975      1.889     0.8973         18        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.22it/s]\n",
            "                   all        100        101      0.447       0.24       0.22     0.0502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100      10.8G      3.019       1.84     0.9038         20        640: 100% 27/27 [00:22<00:00,  1.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.63it/s]\n",
            "                   all        100        101      0.584      0.319      0.313       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100      10.8G        2.9      1.873     0.8919         14        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.73it/s]\n",
            "                   all        100        101      0.456      0.218      0.229     0.0696\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100      11.1G      3.016      1.834     0.8928         24        640: 100% 27/27 [00:30<00:00,  1.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.91it/s]\n",
            "                   all        100        101      0.654      0.366      0.408      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100      11.2G      3.032      1.924     0.9083         17        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.57it/s]\n",
            "                   all        100        101      0.486      0.327      0.288     0.0755\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100      10.8G      3.038        1.8     0.8936         18        640: 100% 27/27 [00:21<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101      0.646      0.277      0.311     0.0688\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100      10.8G      2.787      1.821     0.8739         22        640: 100% 27/27 [00:30<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.97it/s]\n",
            "                   all        100        101      0.744      0.376      0.412      0.131\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100      11.1G      2.863      1.841     0.8939         19        640: 100% 27/27 [00:20<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.88it/s]\n",
            "                   all        100        101      0.561      0.228       0.23      0.069\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100      10.9G      2.973      1.846     0.9029         21        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.44it/s]\n",
            "                   all        100        101      0.601      0.327      0.353      0.105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100      11.1G       2.87      1.657     0.8858         19        640: 100% 27/27 [00:31<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.61it/s]\n",
            "                   all        100        101      0.598      0.347      0.343     0.0952\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100      10.8G      2.937       1.76     0.9002         24        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.63it/s]\n",
            "                   all        100        101      0.448      0.297       0.25     0.0649\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100      11.1G      2.861      1.738     0.8877         18        640: 100% 27/27 [00:30<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.42it/s]\n",
            "                   all        100        101      0.624      0.395       0.39      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100      11.2G      2.769      1.608     0.8686         22        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.73it/s]\n",
            "                   all        100        101       0.64      0.426       0.43      0.122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100      11.1G      2.693      1.677     0.8779         20        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.73it/s]\n",
            "                   all        100        101      0.689      0.417      0.439       0.14\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100      10.8G      2.735      1.619       0.88         20        640: 100% 27/27 [00:21<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.72it/s]\n",
            "                   all        100        101      0.734      0.307      0.377     0.0991\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100      10.8G      2.734      1.651     0.8695         18        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.82it/s]\n",
            "                   all        100        101      0.553      0.344      0.382      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100      10.8G       2.84      1.688     0.8742         15        640: 100% 27/27 [00:20<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.57it/s]\n",
            "                   all        100        101      0.765      0.416      0.488      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100      10.8G      2.752      1.655     0.8709         22        640: 100% 27/27 [00:20<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.35it/s]\n",
            "                   all        100        101      0.511      0.356      0.313     0.0762\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100      11.1G       2.72      1.516     0.8771         23        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.77it/s]\n",
            "                   all        100        101       0.74      0.446      0.449      0.174\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100      11.1G      2.636      1.471     0.8756         26        640: 100% 27/27 [00:22<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  2.00it/s]\n",
            "                   all        100        101      0.609      0.337      0.346      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100      10.8G      2.591      1.566     0.8921         24        640: 100% 27/27 [00:30<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.24it/s]\n",
            "                   all        100        101      0.674      0.287      0.329      0.084\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100      10.8G      2.811      1.692     0.8854         16        640: 100% 27/27 [00:20<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.41it/s]\n",
            "                   all        100        101      0.588      0.426       0.38      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100      11.1G      2.698      1.479     0.8731         18        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.11it/s]\n",
            "                   all        100        101      0.731      0.377      0.418      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100      11.1G      2.587      1.438     0.8625         17        640: 100% 27/27 [00:31<00:00,  1.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.72it/s]\n",
            "                   all        100        101      0.681      0.486       0.51      0.138\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100      10.9G      2.624      1.487     0.8738         18        640: 100% 27/27 [00:22<00:00,  1.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.72it/s]\n",
            "                   all        100        101      0.575      0.327      0.373      0.133\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100      11.1G      2.788      1.394     0.8773         22        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101      0.666      0.416      0.412      0.146\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100      10.8G      2.648      1.471     0.8822         18        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  2.00it/s]\n",
            "                   all        100        101      0.684      0.429      0.444      0.162\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100      11.1G      2.484      1.483     0.8614         21        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.70it/s]\n",
            "                   all        100        101       0.53      0.406      0.389      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100      10.8G      2.619       1.35     0.8839         15        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101       0.67      0.436      0.485      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100      10.8G      2.573      1.495     0.8664         18        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.16it/s]\n",
            "                   all        100        101      0.683      0.363       0.46      0.155\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100      10.8G      2.643       1.42     0.8606         20        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.67it/s]\n",
            "                   all        100        101      0.646      0.396      0.418      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100      11.1G      2.634      1.473     0.8596         23        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.78it/s]\n",
            "                   all        100        101      0.643      0.446      0.482      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100      10.9G      2.499      1.365     0.8385         24        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.11it/s]\n",
            "                   all        100        101      0.816      0.465      0.571      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100      10.8G      2.516      1.331     0.8608         19        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.70it/s]\n",
            "                   all        100        101      0.751      0.455      0.529      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100      10.8G      2.441      1.223     0.8552         21        640: 100% 27/27 [00:20<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.07it/s]\n",
            "                   all        100        101       0.83      0.532      0.563      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100      11.1G      2.455      1.371     0.8445         18        640: 100% 27/27 [00:47<00:00,  1.77s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.96it/s]\n",
            "                   all        100        101      0.702      0.426      0.481       0.13\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100      10.8G      2.493      1.277     0.8487         17        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.95it/s]\n",
            "                   all        100        101      0.588      0.436      0.452      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100      10.8G      2.554      1.237     0.8589         22        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.69it/s]\n",
            "                   all        100        101      0.643      0.347      0.438      0.118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100      10.8G      2.472      1.275     0.8721         25        640: 100% 27/27 [00:30<00:00,  1.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.68it/s]\n",
            "                   all        100        101      0.739      0.545      0.582      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/100      10.8G      2.483      1.408     0.8589         16        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.32it/s]\n",
            "                   all        100        101      0.776      0.356      0.553      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/100      10.9G        2.4       1.28     0.8406         15        640: 100% 27/27 [00:20<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.76it/s]\n",
            "                   all        100        101      0.763      0.542      0.587       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/100      10.8G      2.298      1.284     0.8638         23        640: 100% 27/27 [00:22<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.82it/s]\n",
            "                   all        100        101      0.664      0.495      0.539      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/100      10.8G      2.306      1.203     0.8456         22        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.74it/s]\n",
            "                   all        100        101      0.809      0.547      0.593      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/100      11.1G      2.194      1.174     0.8539         21        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.59it/s]\n",
            "                   all        100        101      0.704      0.542      0.571      0.186\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/100      10.5G      2.317      1.215     0.8435         27        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.85it/s]\n",
            "                   all        100        101      0.782      0.462      0.539      0.196\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/100      10.8G      2.358      1.242     0.8694         17        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.80it/s]\n",
            "                   all        100        101      0.785      0.406      0.528      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/100      10.8G      2.353      1.237     0.8548         20        640: 100% 27/27 [00:29<00:00,  1.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.77it/s]\n",
            "                   all        100        101      0.765      0.548      0.604      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/100      11.1G      2.207      1.206     0.8358         17        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.74it/s]\n",
            "                   all        100        101      0.701      0.505      0.573      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/100      10.9G      2.326      1.226     0.8326         19        640: 100% 27/27 [00:29<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.61it/s]\n",
            "                   all        100        101      0.824      0.464      0.562      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/100      10.8G       2.31      1.175     0.8369         21        640: 100% 27/27 [00:20<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.67it/s]\n",
            "                   all        100        101      0.723      0.525      0.562      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/100      10.8G      2.202      1.095     0.8415         22        640: 100% 27/27 [00:29<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101      0.769      0.535      0.567      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/100      11.1G      2.223      1.096     0.8316         23        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.76it/s]\n",
            "                   all        100        101      0.805      0.489      0.628       0.21\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/100      10.9G      2.272      1.094     0.8546         17        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.71it/s]\n",
            "                   all        100        101      0.754      0.475      0.535      0.183\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/100      10.8G      2.245      1.103     0.8421         21        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.76it/s]\n",
            "                   all        100        101      0.731      0.465      0.522      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/100      10.8G      2.143      1.087     0.8403         25        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.81it/s]\n",
            "                   all        100        101      0.738      0.473      0.554      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/100      11.1G      2.231       1.11      0.847         22        640: 100% 27/27 [00:20<00:00,  1.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.74it/s]\n",
            "                   all        100        101      0.702      0.485      0.526      0.183\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/100      10.9G       2.22      1.086     0.8571         21        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.28it/s]\n",
            "                   all        100        101      0.767      0.522      0.571      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/100      11.1G      2.177      1.076     0.8305         16        640: 100% 27/27 [00:21<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.10it/s]\n",
            "                   all        100        101      0.746      0.545      0.588      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     75/100      10.8G      2.105      1.092     0.8401         16        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.72it/s]\n",
            "                   all        100        101      0.759      0.562      0.597      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     76/100      10.8G      2.156      1.091     0.8457         25        640: 100% 27/27 [00:30<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.64it/s]\n",
            "                   all        100        101      0.776      0.514      0.599      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     77/100      10.9G      2.334      1.165     0.8569         17        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.67it/s]\n",
            "                   all        100        101      0.609      0.447      0.433      0.168\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     78/100      11.1G      2.173      1.105     0.8543         19        640: 100% 27/27 [00:28<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.21it/s]\n",
            "                   all        100        101      0.737      0.526      0.578      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     79/100      10.8G      2.192      1.086     0.8537         11        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.73it/s]\n",
            "                   all        100        101      0.802      0.482      0.612      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     80/100      11.1G       2.13       1.02     0.8341         17        640: 100% 27/27 [00:20<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.77it/s]\n",
            "                   all        100        101      0.774      0.543      0.626      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     81/100      10.9G      2.208      1.126     0.8388         23        640: 100% 27/27 [00:25<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.26it/s]\n",
            "                   all        100        101      0.757      0.545      0.563      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     82/100      10.8G      2.191      1.148     0.8254         17        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101      0.783        0.5      0.545      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     83/100      10.8G      2.204       1.15     0.8462         22        640: 100% 27/27 [00:29<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.72it/s]\n",
            "                   all        100        101       0.82      0.475      0.603      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     84/100      11.1G      2.061      0.942     0.8509         21        640: 100% 27/27 [00:30<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.09it/s]\n",
            "                   all        100        101      0.795        0.5      0.587      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     85/100      10.8G      2.133      0.996     0.8407         16        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.62it/s]\n",
            "                   all        100        101      0.773      0.574      0.628      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     86/100      10.7G      2.134     0.9919     0.8326         20        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.01it/s]\n",
            "                   all        100        101      0.718      0.485      0.586      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     87/100      10.8G      2.126     0.9829     0.8566         23        640: 100% 27/27 [00:20<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.10it/s]\n",
            "                   all        100        101      0.696      0.499      0.591      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     88/100      11.1G      2.061     0.9524       0.85         19        640: 100% 27/27 [00:20<00:00,  1.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.17it/s]\n",
            "                   all        100        101      0.722      0.485      0.592      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     89/100      10.9G      2.221      1.056     0.8419         15        640: 100% 27/27 [00:20<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.79it/s]\n",
            "                   all        100        101      0.761      0.537      0.594      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     90/100      11.1G      2.037     0.9564     0.8281         22        640: 100% 27/27 [00:28<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.68it/s]\n",
            "                   all        100        101      0.732      0.535       0.56      0.215\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     91/100      10.8G      1.997     0.9803      0.836         12        640: 100% 27/27 [00:31<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.18it/s]\n",
            "                   all        100        101      0.707      0.455      0.536      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     92/100      10.8G      1.943     0.9382     0.8495         12        640: 100% 27/27 [00:28<00:00,  1.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.68it/s]\n",
            "                   all        100        101      0.676      0.579       0.59      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     93/100      10.6G      2.104      1.088     0.8551         12        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.75it/s]\n",
            "                   all        100        101      0.647      0.535      0.562      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     94/100      11.1G      1.945     0.9622     0.8424         12        640: 100% 27/27 [00:20<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.65it/s]\n",
            "                   all        100        101      0.646      0.525      0.567      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     95/100      11.1G      2.008     0.9603     0.8206         12        640: 100% 27/27 [00:28<00:00,  1.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  2.00it/s]\n",
            "                   all        100        101      0.682       0.53      0.602      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     96/100      11.1G      1.999     0.9419     0.8512         12        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.71it/s]\n",
            "                   all        100        101      0.706      0.547       0.63      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     97/100      10.8G      1.914     0.8789     0.8536         12        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.03it/s]\n",
            "                   all        100        101      0.724       0.52      0.605       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     98/100      10.8G      1.994     0.9985     0.8525         12        640: 100% 27/27 [00:20<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.05it/s]\n",
            "                   all        100        101      0.747      0.545      0.615      0.249\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     99/100      10.8G      1.861     0.8068     0.8544         12        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.92it/s]\n",
            "                   all        100        101      0.756      0.552      0.619      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    100/100      11.1G      1.944     0.8941     0.8377         12        640: 100% 27/27 [00:29<00:00,  1.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:01<00:00,  2.66it/s]\n",
            "                   all        100        101      0.731      0.483      0.591      0.222\n",
            "\n",
            "100 epochs completed in 1.112 hours.\n",
            "Optimizer stripped from runs/detect/train7/weights/last.pt, 172.6MB\n",
            "Optimizer stripped from runs/detect/train7/weights/best.pt, 172.6MB\n",
            "\n",
            "Validating runs/detect/train7/weights/best.pt...\n",
            "Ultralytics 8.3.162 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv5l6u summary (fused): 167 layers, 85,972,308 parameters, 0 gradients, 137.0 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:02<00:00,  1.80it/s]\n",
            "                   all        100        101      0.706      0.547      0.631      0.267\n",
            "Speed: 0.3ms preprocess, 13.0ms inference, 0.0ms loss, 2.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train7\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=train model=yolov5l6u.pt data='tennis-ball-detection-6/data.yaml' epochs=100 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Â ResNet50 for Court Keypoints detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMrWy6nMM-R5",
        "outputId": "104476cd-cc9c-407f-9b8c-c9f6fb4ac11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-07 00:06:22--  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7255696316 (6.8G) [application/octet-stream]\n",
            "Saving to: â€˜tennis_court_det_dataset.zipâ€™\n",
            "\n",
            "tennis_court_det_da 100%[===================>]   6.76G  44.7MB/s    in 3m 31s  \n",
            "\n",
            "2025-07-07 00:09:55 (32.8 MB/s) - â€˜tennis_court_det_dataset.zipâ€™ saved [7255696316/7255696316]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9,ar;q=0.8\" --header=\"Cookie: HSID=Ag2OIHvsd2Wub4C7z; SSID=AWnBcQKwDHiTrZAU1; APISID=pltrFZgE9lJ0o1gq/AN9feEHYvs8oHd519; SAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-1PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-3PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; SID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-uttBbVDRolhF-hY16nwHXw0gACgYKAWISAQASFQHGX2MivNTw_E_toJuIRy6LMpKNOBoVAUF8yKpFSmvq7AMjvEWeNc50Zff40076; __Secure-1PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utbSY2jBY1VXuw8gYl5hIO2QACgYKAXsSAQASFQHGX2MihVCJ1PwLozGqZgdSatM9QhoVAUF8yKpgrsTvI8i_UE-YHpoN7Gx-0076; __Secure-3PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utwVfPl2imdPimZJ9tdDZGQAACgYKAUESAQASFQHGX2MiEJ49mV4jME2kttDAV5hwWBoVAUF8yKp80mIgju1lu-q4nI7VsFDM0076; NID=511=efI9IZpxtyJ7Dw1MAUXU8FlzS5jXGewY4Er8HliWc3A0RSWdgvNDyKY66ETjgRyTGWPbWODSmiSeYSBab5SPHVwqbJxd6ZeGW2f6BkHi61UKksXPH0CVJRM1hKpMjHPU5qw7tboM2Mi87NrosV8COB-GCLulLLbjOoSAEQewTe8NVZ5Owq8IkwvxFGfJkmUKEMkFWrw9yb5nTDl3wbZEsGFI92iEdNTSxSRovNCIPN2US-SCFdQ0m2BtvwdiWZbgnn7dSQ8yPA145Kk2BA-ATpJNJ6SJHEHLQY-9CPail9D5qgJgxR925EUg5RGCpEu9wS5xbA62KTa19wAvbAq7Dk3TWc-iX4p1s7ESFyDC7yMpFxiFPJjqkWwFi_ZfiK2TW2t0TQ60DFBxqOytQaLyHrkEvD-CQPVj6OCOP22cZY0Cu61HaAQgFO9pXH-kJUlywzVdbirJumN5gswyaQ49b3KdLcG0Jb7brOMTM24T2nGtQ10hJzsnTwX7dBk3ujqQrI_DGuURvPassPUrIZ0; AEC=Ae3NU9MOEGeKAZjP6INpOYbyMraWAWztmx5pJB_1ILu1furiTy1K37k15u0; __Secure-1PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; __Secure-3PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; 1P_JAR=2024-02-18-08; SIDCC=ABTWhQExCxkfmwCkG1RaEgz8U1ZkPeh3HmLMUdMt8S5cNSsLY5U5rAL6wlvq7dtjRw7zrtAbqsFI; __Secure-1PSIDCC=ABTWhQH0jLeRIS6Tu3LS8DXB5Q3gGDq9LTmlk60FKu795Bf0UbzsOcYWVAE96clq5aAL8i724Q0; __Secure-3PSIDCC=ABTWhQHIFcyv3nZYwp78WXEQal71jCE_ZsGT5lXs8VLr7XDIfFqHcLTIPz4HxzJb9ZnYQ5l2s9eU\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\" -c -O 'tennis_court_det_dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gJFVN1kyOBI4"
      },
      "outputs": [],
      "source": [
        "#!unzip tennis_court_det_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smf-gpHnPISA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kpQrwxZEPKSX"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2In2yjJPM6G"
      },
      "outputs": [],
      "source": [
        "class KeypointsDataset(Dataset):\n",
        "    def __init__(self, img_dir, data_file):\n",
        "        self.img_dir = img_dir\n",
        "        with open(data_file, \"r\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
        "        h,w = img.shape[:2]\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self.transforms(img)\n",
        "        kps = np.array(item['kps']).flatten()\n",
        "        kps = kps.astype(np.float32)\n",
        "\n",
        "        kps[::2] *= 224.0 / w # Adjust x coordinates\n",
        "        kps[1::2] *= 224.0 / h # Adjust y coordinates\n",
        "\n",
        "        return img, kps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MdQjO0_-POyV"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = KeypointsDataset(\"data/images\",\"data/data_train.json\")\n",
        "val_dataset = KeypointsDataset(\"data/images\",\"data/data_val.json\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfXG4DBJPQF9",
        "outputId": "8c419aad-c355-469f-f3b7-a16500b83873"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 11,577,692\n",
            "Trainable parameters: 11,577,692\n",
            "[Epoch 00] Batch 001/829 | Train Loss: 109.6672\n",
            "[Epoch 00] Batch 011/829 | Train Loss: 109.9804\n",
            "[Epoch 00] Batch 021/829 | Train Loss: 112.7408\n",
            "[Epoch 00] Batch 031/829 | Train Loss: 111.5859\n",
            "[Epoch 00] Batch 041/829 | Train Loss: 106.0374\n",
            "[Epoch 00] Batch 051/829 | Train Loss: 103.5850\n",
            "[Epoch 00] Batch 061/829 | Train Loss: 98.8786\n",
            "[Epoch 00] Batch 071/829 | Train Loss: 86.2942\n",
            "[Epoch 00] Batch 081/829 | Train Loss: 67.9700\n",
            "[Epoch 00] Batch 091/829 | Train Loss: 49.5132\n",
            "[Epoch 00] Batch 101/829 | Train Loss: 41.0063\n",
            "[Epoch 00] Batch 111/829 | Train Loss: 29.6024\n",
            "[Epoch 00] Batch 121/829 | Train Loss: 19.4669\n",
            "[Epoch 00] Batch 131/829 | Train Loss: 13.8086\n",
            "[Epoch 00] Batch 141/829 | Train Loss: 10.8618\n",
            "[Epoch 00] Batch 151/829 | Train Loss: 12.8645\n",
            "[Epoch 00] Batch 161/829 | Train Loss: 13.6577\n",
            "[Epoch 00] Batch 171/829 | Train Loss: 12.9691\n",
            "[Epoch 00] Batch 181/829 | Train Loss: 12.8223\n",
            "[Epoch 00] Batch 191/829 | Train Loss: 13.5331\n",
            "[Epoch 00] Batch 201/829 | Train Loss: 13.4036\n",
            "[Epoch 00] Batch 211/829 | Train Loss: 11.1239\n",
            "[Epoch 00] Batch 221/829 | Train Loss: 14.2443\n",
            "[Epoch 00] Batch 231/829 | Train Loss: 11.2322\n",
            "[Epoch 00] Batch 241/829 | Train Loss: 11.2448\n",
            "[Epoch 00] Batch 251/829 | Train Loss: 11.3467\n",
            "[Epoch 00] Batch 261/829 | Train Loss: 13.5192\n",
            "[Epoch 00] Batch 271/829 | Train Loss: 13.1981\n",
            "[Epoch 00] Batch 281/829 | Train Loss: 11.2309\n",
            "[Epoch 00] Batch 291/829 | Train Loss: 10.4477\n",
            "[Epoch 00] Batch 301/829 | Train Loss: 11.7013\n",
            "[Epoch 00] Batch 311/829 | Train Loss: 10.1451\n",
            "[Epoch 00] Batch 321/829 | Train Loss: 11.0820\n",
            "[Epoch 00] Batch 331/829 | Train Loss: 10.8901\n",
            "[Epoch 00] Batch 341/829 | Train Loss: 10.9022\n",
            "[Epoch 00] Batch 351/829 | Train Loss: 10.7837\n",
            "[Epoch 00] Batch 361/829 | Train Loss: 10.2466\n",
            "[Epoch 00] Batch 371/829 | Train Loss: 10.7656\n",
            "[Epoch 00] Batch 381/829 | Train Loss: 10.3756\n",
            "[Epoch 00] Batch 391/829 | Train Loss: 9.6636\n",
            "[Epoch 00] Batch 401/829 | Train Loss: 9.8273\n",
            "[Epoch 00] Batch 411/829 | Train Loss: 12.3688\n",
            "[Epoch 00] Batch 421/829 | Train Loss: 9.7561\n",
            "[Epoch 00] Batch 431/829 | Train Loss: 9.4588\n",
            "[Epoch 00] Batch 441/829 | Train Loss: 11.3745\n",
            "[Epoch 00] Batch 451/829 | Train Loss: 10.6046\n",
            "[Epoch 00] Batch 461/829 | Train Loss: 11.7918\n",
            "[Epoch 00] Batch 471/829 | Train Loss: 10.0518\n",
            "[Epoch 00] Batch 481/829 | Train Loss: 11.2993\n",
            "[Epoch 00] Batch 491/829 | Train Loss: 9.7775\n",
            "[Epoch 00] Batch 501/829 | Train Loss: 11.0899\n",
            "[Epoch 00] Batch 511/829 | Train Loss: 11.2530\n",
            "[Epoch 00] Batch 521/829 | Train Loss: 12.9988\n",
            "[Epoch 00] Batch 531/829 | Train Loss: 12.0501\n",
            "[Epoch 00] Batch 541/829 | Train Loss: 11.2638\n",
            "[Epoch 00] Batch 551/829 | Train Loss: 9.0460\n",
            "[Epoch 00] Batch 561/829 | Train Loss: 11.1147\n",
            "[Epoch 00] Batch 571/829 | Train Loss: 11.4838\n",
            "[Epoch 00] Batch 581/829 | Train Loss: 11.6573\n",
            "[Epoch 00] Batch 591/829 | Train Loss: 9.7828\n",
            "[Epoch 00] Batch 601/829 | Train Loss: 10.6918\n",
            "[Epoch 00] Batch 611/829 | Train Loss: 10.6697\n",
            "[Epoch 00] Batch 621/829 | Train Loss: 10.2752\n",
            "[Epoch 00] Batch 631/829 | Train Loss: 10.1625\n",
            "[Epoch 00] Batch 641/829 | Train Loss: 9.5388\n",
            "[Epoch 00] Batch 651/829 | Train Loss: 13.5565\n",
            "[Epoch 00] Batch 661/829 | Train Loss: 11.6469\n",
            "[Epoch 00] Batch 671/829 | Train Loss: 9.4165\n",
            "[Epoch 00] Batch 681/829 | Train Loss: 8.4789\n",
            "[Epoch 00] Batch 691/829 | Train Loss: 10.1611\n",
            "[Epoch 00] Batch 701/829 | Train Loss: 8.7892\n",
            "[Epoch 00] Batch 711/829 | Train Loss: 11.7577\n",
            "[Epoch 00] Batch 721/829 | Train Loss: 12.4233\n",
            "[Epoch 00] Batch 731/829 | Train Loss: 12.8378\n",
            "[Epoch 00] Batch 741/829 | Train Loss: 9.2685\n",
            "[Epoch 00] Batch 751/829 | Train Loss: 13.0650\n",
            "[Epoch 00] Batch 761/829 | Train Loss: 9.7113\n",
            "[Epoch 00] Batch 771/829 | Train Loss: 11.2517\n",
            "[Epoch 00] Batch 781/829 | Train Loss: 11.9852\n",
            "[Epoch 00] Batch 791/829 | Train Loss: 10.4808\n",
            "[Epoch 00] Batch 801/829 | Train Loss: 8.7922\n",
            "[Epoch 00] Batch 811/829 | Train Loss: 7.8900\n",
            "[Epoch 00] Batch 821/829 | Train Loss: 10.1892\n",
            "[Epoch 00] Batch 829/829 | Train Loss: 9.1406\n",
            "ðŸŸ¢ Epoch 00 | Avg Train Loss: 21.3384 | Val Loss: 5.0883\n",
            "âœ… Best model saved: models/best_model_resnet18_mlp_aug_smoothl1.pth (Val Loss = 5.0883)\n",
            "ðŸ’¾ Checkpoint saved at epoch 1: models/checkpoint_epoch_01_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 01] Batch 001/829 | Train Loss: 13.5690\n",
            "[Epoch 01] Batch 011/829 | Train Loss: 11.6993\n",
            "[Epoch 01] Batch 021/829 | Train Loss: 10.4538\n",
            "[Epoch 01] Batch 031/829 | Train Loss: 8.6403\n",
            "[Epoch 01] Batch 041/829 | Train Loss: 9.9091\n",
            "[Epoch 01] Batch 051/829 | Train Loss: 10.3272\n",
            "[Epoch 01] Batch 061/829 | Train Loss: 9.1273\n",
            "[Epoch 01] Batch 071/829 | Train Loss: 9.2305\n",
            "[Epoch 01] Batch 081/829 | Train Loss: 8.6114\n",
            "[Epoch 01] Batch 091/829 | Train Loss: 10.4931\n",
            "[Epoch 01] Batch 101/829 | Train Loss: 9.6944\n",
            "[Epoch 01] Batch 111/829 | Train Loss: 10.6877\n",
            "[Epoch 01] Batch 121/829 | Train Loss: 8.2728\n",
            "[Epoch 01] Batch 131/829 | Train Loss: 10.2626\n",
            "[Epoch 01] Batch 141/829 | Train Loss: 8.5882\n",
            "[Epoch 01] Batch 151/829 | Train Loss: 9.6678\n",
            "[Epoch 01] Batch 161/829 | Train Loss: 9.0960\n",
            "[Epoch 01] Batch 171/829 | Train Loss: 12.5822\n",
            "[Epoch 01] Batch 181/829 | Train Loss: 10.1489\n",
            "[Epoch 01] Batch 191/829 | Train Loss: 9.6062\n",
            "[Epoch 01] Batch 201/829 | Train Loss: 10.2924\n",
            "[Epoch 01] Batch 211/829 | Train Loss: 10.2752\n",
            "[Epoch 01] Batch 221/829 | Train Loss: 9.2821\n",
            "[Epoch 01] Batch 231/829 | Train Loss: 9.5172\n",
            "[Epoch 01] Batch 241/829 | Train Loss: 9.1505\n",
            "[Epoch 01] Batch 251/829 | Train Loss: 9.1688\n",
            "[Epoch 01] Batch 261/829 | Train Loss: 9.3812\n",
            "[Epoch 01] Batch 271/829 | Train Loss: 9.9730\n",
            "[Epoch 01] Batch 281/829 | Train Loss: 9.7667\n",
            "[Epoch 01] Batch 291/829 | Train Loss: 11.7603\n",
            "[Epoch 01] Batch 301/829 | Train Loss: 10.0972\n",
            "[Epoch 01] Batch 311/829 | Train Loss: 8.6456\n",
            "[Epoch 01] Batch 321/829 | Train Loss: 9.3761\n",
            "[Epoch 01] Batch 331/829 | Train Loss: 8.3365\n",
            "[Epoch 01] Batch 341/829 | Train Loss: 9.1761\n",
            "[Epoch 01] Batch 351/829 | Train Loss: 10.4460\n",
            "[Epoch 01] Batch 361/829 | Train Loss: 9.7082\n",
            "[Epoch 01] Batch 371/829 | Train Loss: 9.0406\n",
            "[Epoch 01] Batch 381/829 | Train Loss: 9.2530\n",
            "[Epoch 01] Batch 391/829 | Train Loss: 8.5201\n",
            "[Epoch 01] Batch 401/829 | Train Loss: 9.2194\n",
            "[Epoch 01] Batch 411/829 | Train Loss: 9.3292\n",
            "[Epoch 01] Batch 421/829 | Train Loss: 8.9981\n",
            "[Epoch 01] Batch 431/829 | Train Loss: 10.5445\n",
            "[Epoch 01] Batch 441/829 | Train Loss: 9.7068\n",
            "[Epoch 01] Batch 451/829 | Train Loss: 9.0827\n",
            "[Epoch 01] Batch 461/829 | Train Loss: 8.5891\n",
            "[Epoch 01] Batch 471/829 | Train Loss: 8.7878\n",
            "[Epoch 01] Batch 481/829 | Train Loss: 8.3882\n",
            "[Epoch 01] Batch 491/829 | Train Loss: 8.3376\n",
            "[Epoch 01] Batch 501/829 | Train Loss: 8.3538\n",
            "[Epoch 01] Batch 511/829 | Train Loss: 9.3032\n",
            "[Epoch 01] Batch 521/829 | Train Loss: 10.1611\n",
            "[Epoch 01] Batch 531/829 | Train Loss: 8.9691\n",
            "[Epoch 01] Batch 541/829 | Train Loss: 9.8563\n",
            "[Epoch 01] Batch 551/829 | Train Loss: 10.5296\n",
            "[Epoch 01] Batch 561/829 | Train Loss: 7.9689\n",
            "[Epoch 01] Batch 571/829 | Train Loss: 8.3933\n",
            "[Epoch 01] Batch 581/829 | Train Loss: 9.9636\n",
            "[Epoch 01] Batch 591/829 | Train Loss: 10.8806\n",
            "[Epoch 01] Batch 601/829 | Train Loss: 12.8993\n",
            "[Epoch 01] Batch 611/829 | Train Loss: 9.0213\n",
            "[Epoch 01] Batch 621/829 | Train Loss: 7.0078\n",
            "[Epoch 01] Batch 631/829 | Train Loss: 9.4313\n",
            "[Epoch 01] Batch 641/829 | Train Loss: 9.7502\n",
            "[Epoch 01] Batch 651/829 | Train Loss: 9.4807\n",
            "[Epoch 01] Batch 661/829 | Train Loss: 7.5461\n",
            "[Epoch 01] Batch 671/829 | Train Loss: 9.6547\n",
            "[Epoch 01] Batch 681/829 | Train Loss: 10.2805\n",
            "[Epoch 01] Batch 691/829 | Train Loss: 11.5145\n",
            "[Epoch 01] Batch 701/829 | Train Loss: 9.0073\n",
            "[Epoch 01] Batch 711/829 | Train Loss: 10.5099\n",
            "[Epoch 01] Batch 721/829 | Train Loss: 8.1668\n",
            "[Epoch 01] Batch 731/829 | Train Loss: 7.5570\n",
            "[Epoch 01] Batch 741/829 | Train Loss: 10.0894\n",
            "[Epoch 01] Batch 751/829 | Train Loss: 7.9775\n",
            "[Epoch 01] Batch 761/829 | Train Loss: 9.4457\n",
            "[Epoch 01] Batch 771/829 | Train Loss: 9.9484\n",
            "[Epoch 01] Batch 781/829 | Train Loss: 11.0315\n",
            "[Epoch 01] Batch 791/829 | Train Loss: 8.0133\n",
            "[Epoch 01] Batch 801/829 | Train Loss: 10.0052\n",
            "[Epoch 01] Batch 811/829 | Train Loss: 9.8506\n",
            "[Epoch 01] Batch 821/829 | Train Loss: 8.2443\n",
            "[Epoch 01] Batch 829/829 | Train Loss: 8.1659\n",
            "ðŸŸ¢ Epoch 01 | Avg Train Loss: 9.5773 | Val Loss: 4.6678\n",
            "âœ… Best model saved: models/best_model_resnet18_mlp_aug_smoothl1.pth (Val Loss = 4.6678)\n",
            "ðŸ’¾ Checkpoint saved at epoch 2: models/checkpoint_epoch_02_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 02] Batch 001/829 | Train Loss: 8.8831\n",
            "[Epoch 02] Batch 011/829 | Train Loss: 9.3234\n",
            "[Epoch 02] Batch 021/829 | Train Loss: 8.4249\n",
            "[Epoch 02] Batch 031/829 | Train Loss: 10.0886\n",
            "[Epoch 02] Batch 041/829 | Train Loss: 7.4235\n",
            "[Epoch 02] Batch 051/829 | Train Loss: 9.1705\n",
            "[Epoch 02] Batch 061/829 | Train Loss: 7.6232\n",
            "[Epoch 02] Batch 071/829 | Train Loss: 7.3022\n",
            "[Epoch 02] Batch 081/829 | Train Loss: 7.9443\n",
            "[Epoch 02] Batch 091/829 | Train Loss: 9.7656\n",
            "[Epoch 02] Batch 101/829 | Train Loss: 9.8927\n",
            "[Epoch 02] Batch 111/829 | Train Loss: 8.4785\n",
            "[Epoch 02] Batch 121/829 | Train Loss: 7.4361\n",
            "[Epoch 02] Batch 131/829 | Train Loss: 8.2891\n",
            "[Epoch 02] Batch 141/829 | Train Loss: 8.6240\n",
            "[Epoch 02] Batch 151/829 | Train Loss: 6.2750\n",
            "[Epoch 02] Batch 161/829 | Train Loss: 7.8432\n",
            "[Epoch 02] Batch 171/829 | Train Loss: 9.5820\n",
            "[Epoch 02] Batch 181/829 | Train Loss: 8.9237\n",
            "[Epoch 02] Batch 191/829 | Train Loss: 10.0333\n",
            "[Epoch 02] Batch 201/829 | Train Loss: 9.1717\n",
            "[Epoch 02] Batch 211/829 | Train Loss: 8.3277\n",
            "[Epoch 02] Batch 221/829 | Train Loss: 8.2299\n",
            "[Epoch 02] Batch 231/829 | Train Loss: 7.8926\n",
            "[Epoch 02] Batch 241/829 | Train Loss: 8.4329\n",
            "[Epoch 02] Batch 251/829 | Train Loss: 8.9698\n",
            "[Epoch 02] Batch 261/829 | Train Loss: 8.5118\n",
            "[Epoch 02] Batch 271/829 | Train Loss: 10.6927\n",
            "[Epoch 02] Batch 281/829 | Train Loss: 10.7633\n",
            "[Epoch 02] Batch 291/829 | Train Loss: 7.5614\n",
            "[Epoch 02] Batch 301/829 | Train Loss: 9.8845\n",
            "[Epoch 02] Batch 311/829 | Train Loss: 10.9268\n",
            "[Epoch 02] Batch 321/829 | Train Loss: 9.3008\n",
            "[Epoch 02] Batch 331/829 | Train Loss: 8.2019\n",
            "[Epoch 02] Batch 341/829 | Train Loss: 7.9569\n",
            "[Epoch 02] Batch 351/829 | Train Loss: 9.9580\n",
            "[Epoch 02] Batch 361/829 | Train Loss: 9.0615\n",
            "[Epoch 02] Batch 371/829 | Train Loss: 7.8007\n",
            "[Epoch 02] Batch 381/829 | Train Loss: 8.4079\n",
            "[Epoch 02] Batch 391/829 | Train Loss: 8.7215\n",
            "[Epoch 02] Batch 401/829 | Train Loss: 8.4798\n",
            "[Epoch 02] Batch 411/829 | Train Loss: 8.9272\n",
            "[Epoch 02] Batch 421/829 | Train Loss: 9.1013\n",
            "[Epoch 02] Batch 431/829 | Train Loss: 8.7721\n",
            "[Epoch 02] Batch 441/829 | Train Loss: 9.3401\n",
            "[Epoch 02] Batch 451/829 | Train Loss: 8.9141\n",
            "[Epoch 02] Batch 461/829 | Train Loss: 7.2116\n",
            "[Epoch 02] Batch 471/829 | Train Loss: 10.5744\n",
            "[Epoch 02] Batch 481/829 | Train Loss: 7.5126\n",
            "[Epoch 02] Batch 491/829 | Train Loss: 9.0799\n",
            "[Epoch 02] Batch 501/829 | Train Loss: 8.3252\n",
            "[Epoch 02] Batch 511/829 | Train Loss: 10.0913\n",
            "[Epoch 02] Batch 521/829 | Train Loss: 10.1506\n",
            "[Epoch 02] Batch 531/829 | Train Loss: 6.1094\n",
            "[Epoch 02] Batch 541/829 | Train Loss: 12.1294\n",
            "[Epoch 02] Batch 551/829 | Train Loss: 8.8644\n",
            "[Epoch 02] Batch 561/829 | Train Loss: 6.8473\n",
            "[Epoch 02] Batch 571/829 | Train Loss: 7.5349\n",
            "[Epoch 02] Batch 581/829 | Train Loss: 8.7685\n",
            "[Epoch 02] Batch 591/829 | Train Loss: 8.3196\n",
            "[Epoch 02] Batch 601/829 | Train Loss: 9.1683\n",
            "[Epoch 02] Batch 611/829 | Train Loss: 9.7864\n",
            "[Epoch 02] Batch 621/829 | Train Loss: 8.2643\n",
            "[Epoch 02] Batch 631/829 | Train Loss: 8.4553\n",
            "[Epoch 02] Batch 641/829 | Train Loss: 10.0763\n",
            "[Epoch 02] Batch 651/829 | Train Loss: 9.6497\n",
            "[Epoch 02] Batch 661/829 | Train Loss: 10.5153\n",
            "[Epoch 02] Batch 671/829 | Train Loss: 9.9851\n",
            "[Epoch 02] Batch 681/829 | Train Loss: 9.9318\n",
            "[Epoch 02] Batch 691/829 | Train Loss: 8.1083\n",
            "[Epoch 02] Batch 701/829 | Train Loss: 11.4094\n",
            "[Epoch 02] Batch 711/829 | Train Loss: 9.3479\n",
            "[Epoch 02] Batch 721/829 | Train Loss: 11.5629\n",
            "[Epoch 02] Batch 731/829 | Train Loss: 5.8433\n",
            "[Epoch 02] Batch 741/829 | Train Loss: 8.2779\n",
            "[Epoch 02] Batch 751/829 | Train Loss: 6.8730\n",
            "[Epoch 02] Batch 761/829 | Train Loss: 7.8571\n",
            "[Epoch 02] Batch 771/829 | Train Loss: 8.2316\n",
            "[Epoch 02] Batch 781/829 | Train Loss: 10.0511\n",
            "[Epoch 02] Batch 791/829 | Train Loss: 9.2750\n",
            "[Epoch 02] Batch 801/829 | Train Loss: 7.4587\n",
            "[Epoch 02] Batch 811/829 | Train Loss: 11.1478\n",
            "[Epoch 02] Batch 821/829 | Train Loss: 8.7907\n",
            "[Epoch 02] Batch 829/829 | Train Loss: 7.3579\n",
            "ðŸŸ¢ Epoch 02 | Avg Train Loss: 8.8555 | Val Loss: 4.8526\n",
            "ðŸ’¾ Checkpoint saved at epoch 3: models/checkpoint_epoch_03_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 03] Batch 001/829 | Train Loss: 10.4226\n",
            "[Epoch 03] Batch 011/829 | Train Loss: 7.5676\n",
            "[Epoch 03] Batch 021/829 | Train Loss: 8.6065\n",
            "[Epoch 03] Batch 031/829 | Train Loss: 6.3647\n",
            "[Epoch 03] Batch 041/829 | Train Loss: 7.0863\n",
            "[Epoch 03] Batch 051/829 | Train Loss: 7.7649\n",
            "[Epoch 03] Batch 061/829 | Train Loss: 7.5968\n",
            "[Epoch 03] Batch 071/829 | Train Loss: 7.7945\n",
            "[Epoch 03] Batch 081/829 | Train Loss: 9.1007\n",
            "[Epoch 03] Batch 091/829 | Train Loss: 7.9790\n",
            "[Epoch 03] Batch 101/829 | Train Loss: 9.7273\n",
            "[Epoch 03] Batch 111/829 | Train Loss: 10.9867\n",
            "[Epoch 03] Batch 121/829 | Train Loss: 9.7840\n",
            "[Epoch 03] Batch 131/829 | Train Loss: 9.5002\n",
            "[Epoch 03] Batch 141/829 | Train Loss: 7.1115\n",
            "[Epoch 03] Batch 151/829 | Train Loss: 6.7499\n",
            "[Epoch 03] Batch 161/829 | Train Loss: 10.4506\n",
            "[Epoch 03] Batch 171/829 | Train Loss: 8.2358\n",
            "[Epoch 03] Batch 181/829 | Train Loss: 7.3538\n",
            "[Epoch 03] Batch 191/829 | Train Loss: 8.0758\n",
            "[Epoch 03] Batch 201/829 | Train Loss: 9.1681\n",
            "[Epoch 03] Batch 211/829 | Train Loss: 8.3043\n",
            "[Epoch 03] Batch 221/829 | Train Loss: 9.6048\n",
            "[Epoch 03] Batch 231/829 | Train Loss: 8.8938\n",
            "[Epoch 03] Batch 241/829 | Train Loss: 9.9456\n",
            "[Epoch 03] Batch 251/829 | Train Loss: 7.4540\n",
            "[Epoch 03] Batch 261/829 | Train Loss: 9.1007\n",
            "[Epoch 03] Batch 271/829 | Train Loss: 11.6977\n",
            "[Epoch 03] Batch 281/829 | Train Loss: 8.9119\n",
            "[Epoch 03] Batch 291/829 | Train Loss: 8.0875\n",
            "[Epoch 03] Batch 301/829 | Train Loss: 8.2445\n",
            "[Epoch 03] Batch 311/829 | Train Loss: 9.2489\n",
            "[Epoch 03] Batch 321/829 | Train Loss: 9.4306\n",
            "[Epoch 03] Batch 331/829 | Train Loss: 7.2192\n",
            "[Epoch 03] Batch 341/829 | Train Loss: 8.4577\n",
            "[Epoch 03] Batch 351/829 | Train Loss: 7.8536\n",
            "[Epoch 03] Batch 361/829 | Train Loss: 10.0735\n",
            "[Epoch 03] Batch 371/829 | Train Loss: 9.8873\n",
            "[Epoch 03] Batch 381/829 | Train Loss: 8.4236\n",
            "[Epoch 03] Batch 391/829 | Train Loss: 7.5873\n",
            "[Epoch 03] Batch 401/829 | Train Loss: 8.6869\n",
            "[Epoch 03] Batch 411/829 | Train Loss: 8.8891\n",
            "[Epoch 03] Batch 421/829 | Train Loss: 9.8093\n",
            "[Epoch 03] Batch 431/829 | Train Loss: 10.1274\n",
            "[Epoch 03] Batch 441/829 | Train Loss: 8.1588\n",
            "[Epoch 03] Batch 451/829 | Train Loss: 8.3013\n",
            "[Epoch 03] Batch 461/829 | Train Loss: 8.1601\n",
            "[Epoch 03] Batch 471/829 | Train Loss: 7.9500\n",
            "[Epoch 03] Batch 481/829 | Train Loss: 9.5513\n",
            "[Epoch 03] Batch 491/829 | Train Loss: 6.1072\n",
            "[Epoch 03] Batch 501/829 | Train Loss: 11.0774\n",
            "[Epoch 03] Batch 511/829 | Train Loss: 8.6346\n",
            "[Epoch 03] Batch 521/829 | Train Loss: 7.1609\n",
            "[Epoch 03] Batch 531/829 | Train Loss: 8.3831\n",
            "[Epoch 03] Batch 541/829 | Train Loss: 7.6167\n",
            "[Epoch 03] Batch 551/829 | Train Loss: 9.5823\n",
            "[Epoch 03] Batch 561/829 | Train Loss: 7.9950\n",
            "[Epoch 03] Batch 571/829 | Train Loss: 7.2199\n",
            "[Epoch 03] Batch 581/829 | Train Loss: 7.5423\n",
            "[Epoch 03] Batch 591/829 | Train Loss: 7.7649\n",
            "[Epoch 03] Batch 601/829 | Train Loss: 10.8037\n",
            "[Epoch 03] Batch 611/829 | Train Loss: 8.4582\n",
            "[Epoch 03] Batch 621/829 | Train Loss: 8.1964\n",
            "[Epoch 03] Batch 631/829 | Train Loss: 8.0119\n",
            "[Epoch 03] Batch 641/829 | Train Loss: 7.0435\n",
            "[Epoch 03] Batch 651/829 | Train Loss: 7.3185\n",
            "[Epoch 03] Batch 661/829 | Train Loss: 8.6428\n",
            "[Epoch 03] Batch 671/829 | Train Loss: 6.3676\n",
            "[Epoch 03] Batch 681/829 | Train Loss: 8.9137\n",
            "[Epoch 03] Batch 691/829 | Train Loss: 7.1591\n",
            "[Epoch 03] Batch 701/829 | Train Loss: 7.6113\n",
            "[Epoch 03] Batch 711/829 | Train Loss: 7.2942\n",
            "[Epoch 03] Batch 721/829 | Train Loss: 11.9810\n",
            "[Epoch 03] Batch 731/829 | Train Loss: 9.3832\n",
            "[Epoch 03] Batch 741/829 | Train Loss: 9.7446\n",
            "[Epoch 03] Batch 751/829 | Train Loss: 8.7411\n",
            "[Epoch 03] Batch 761/829 | Train Loss: 6.0041\n",
            "[Epoch 03] Batch 771/829 | Train Loss: 7.0668\n",
            "[Epoch 03] Batch 781/829 | Train Loss: 8.9864\n",
            "[Epoch 03] Batch 791/829 | Train Loss: 8.3489\n",
            "[Epoch 03] Batch 801/829 | Train Loss: 8.6495\n",
            "[Epoch 03] Batch 811/829 | Train Loss: 8.8567\n",
            "[Epoch 03] Batch 821/829 | Train Loss: 8.9846\n",
            "[Epoch 03] Batch 829/829 | Train Loss: 9.2362\n",
            "ðŸŸ¢ Epoch 03 | Avg Train Loss: 8.4635 | Val Loss: 4.5819\n",
            "âœ… Best model saved: models/best_model_resnet18_mlp_aug_smoothl1.pth (Val Loss = 4.5819)\n",
            "ðŸ’¾ Checkpoint saved at epoch 4: models/checkpoint_epoch_04_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 04] Batch 001/829 | Train Loss: 7.3586\n",
            "[Epoch 04] Batch 011/829 | Train Loss: 8.3612\n",
            "[Epoch 04] Batch 021/829 | Train Loss: 8.4062\n",
            "[Epoch 04] Batch 031/829 | Train Loss: 7.5692\n",
            "[Epoch 04] Batch 041/829 | Train Loss: 6.6297\n",
            "[Epoch 04] Batch 051/829 | Train Loss: 10.1707\n",
            "[Epoch 04] Batch 061/829 | Train Loss: 7.7646\n",
            "[Epoch 04] Batch 071/829 | Train Loss: 7.3931\n",
            "[Epoch 04] Batch 081/829 | Train Loss: 7.9869\n",
            "[Epoch 04] Batch 091/829 | Train Loss: 7.1625\n",
            "[Epoch 04] Batch 101/829 | Train Loss: 6.7040\n",
            "[Epoch 04] Batch 111/829 | Train Loss: 8.3214\n",
            "[Epoch 04] Batch 121/829 | Train Loss: 8.0331\n",
            "[Epoch 04] Batch 131/829 | Train Loss: 7.4402\n",
            "[Epoch 04] Batch 141/829 | Train Loss: 9.7920\n",
            "[Epoch 04] Batch 151/829 | Train Loss: 7.6627\n",
            "[Epoch 04] Batch 161/829 | Train Loss: 7.2821\n",
            "[Epoch 04] Batch 171/829 | Train Loss: 7.5303\n",
            "[Epoch 04] Batch 181/829 | Train Loss: 13.3082\n",
            "[Epoch 04] Batch 191/829 | Train Loss: 8.2892\n",
            "[Epoch 04] Batch 201/829 | Train Loss: 6.8616\n",
            "[Epoch 04] Batch 211/829 | Train Loss: 7.9885\n",
            "[Epoch 04] Batch 221/829 | Train Loss: 8.7222\n",
            "[Epoch 04] Batch 231/829 | Train Loss: 9.6715\n",
            "[Epoch 04] Batch 241/829 | Train Loss: 8.6044\n",
            "[Epoch 04] Batch 251/829 | Train Loss: 7.2705\n",
            "[Epoch 04] Batch 261/829 | Train Loss: 10.1502\n",
            "[Epoch 04] Batch 271/829 | Train Loss: 7.5699\n",
            "[Epoch 04] Batch 281/829 | Train Loss: 10.2861\n",
            "[Epoch 04] Batch 291/829 | Train Loss: 7.2669\n",
            "[Epoch 04] Batch 301/829 | Train Loss: 8.7994\n",
            "[Epoch 04] Batch 311/829 | Train Loss: 10.5557\n",
            "[Epoch 04] Batch 321/829 | Train Loss: 10.1445\n",
            "[Epoch 04] Batch 331/829 | Train Loss: 7.0532\n",
            "[Epoch 04] Batch 341/829 | Train Loss: 9.2661\n",
            "[Epoch 04] Batch 351/829 | Train Loss: 7.2198\n",
            "[Epoch 04] Batch 361/829 | Train Loss: 7.0437\n",
            "[Epoch 04] Batch 371/829 | Train Loss: 7.7886\n",
            "[Epoch 04] Batch 381/829 | Train Loss: 10.4678\n",
            "[Epoch 04] Batch 391/829 | Train Loss: 8.4468\n",
            "[Epoch 04] Batch 401/829 | Train Loss: 7.7397\n",
            "[Epoch 04] Batch 411/829 | Train Loss: 8.7801\n",
            "[Epoch 04] Batch 421/829 | Train Loss: 6.7993\n",
            "[Epoch 04] Batch 431/829 | Train Loss: 7.0085\n",
            "[Epoch 04] Batch 441/829 | Train Loss: 9.5478\n",
            "[Epoch 04] Batch 451/829 | Train Loss: 5.7411\n",
            "[Epoch 04] Batch 461/829 | Train Loss: 8.5217\n",
            "[Epoch 04] Batch 471/829 | Train Loss: 7.4021\n",
            "[Epoch 04] Batch 481/829 | Train Loss: 8.2319\n",
            "[Epoch 04] Batch 491/829 | Train Loss: 11.2514\n",
            "[Epoch 04] Batch 501/829 | Train Loss: 7.4489\n",
            "[Epoch 04] Batch 511/829 | Train Loss: 7.4662\n",
            "[Epoch 04] Batch 521/829 | Train Loss: 9.0549\n",
            "[Epoch 04] Batch 531/829 | Train Loss: 6.5979\n",
            "[Epoch 04] Batch 541/829 | Train Loss: 9.4561\n",
            "[Epoch 04] Batch 551/829 | Train Loss: 7.9484\n",
            "[Epoch 04] Batch 561/829 | Train Loss: 11.4925\n",
            "[Epoch 04] Batch 571/829 | Train Loss: 10.3165\n",
            "[Epoch 04] Batch 581/829 | Train Loss: 8.5642\n",
            "[Epoch 04] Batch 591/829 | Train Loss: 7.7311\n",
            "[Epoch 04] Batch 601/829 | Train Loss: 6.0692\n",
            "[Epoch 04] Batch 611/829 | Train Loss: 7.6076\n",
            "[Epoch 04] Batch 621/829 | Train Loss: 9.5458\n",
            "[Epoch 04] Batch 631/829 | Train Loss: 7.8698\n",
            "[Epoch 04] Batch 641/829 | Train Loss: 6.9344\n",
            "[Epoch 04] Batch 651/829 | Train Loss: 7.9874\n",
            "[Epoch 04] Batch 661/829 | Train Loss: 5.4652\n",
            "[Epoch 04] Batch 671/829 | Train Loss: 6.7408\n",
            "[Epoch 04] Batch 681/829 | Train Loss: 7.6932\n",
            "[Epoch 04] Batch 691/829 | Train Loss: 7.7446\n",
            "[Epoch 04] Batch 701/829 | Train Loss: 6.9912\n",
            "[Epoch 04] Batch 711/829 | Train Loss: 8.6472\n",
            "[Epoch 04] Batch 721/829 | Train Loss: 8.1718\n",
            "[Epoch 04] Batch 731/829 | Train Loss: 6.5923\n",
            "[Epoch 04] Batch 741/829 | Train Loss: 9.0211\n",
            "[Epoch 04] Batch 751/829 | Train Loss: 8.0050\n",
            "[Epoch 04] Batch 761/829 | Train Loss: 8.7412\n",
            "[Epoch 04] Batch 771/829 | Train Loss: 7.5835\n",
            "[Epoch 04] Batch 781/829 | Train Loss: 7.7337\n",
            "[Epoch 04] Batch 791/829 | Train Loss: 7.9195\n",
            "[Epoch 04] Batch 801/829 | Train Loss: 7.6299\n",
            "[Epoch 04] Batch 811/829 | Train Loss: 5.7760\n",
            "[Epoch 04] Batch 821/829 | Train Loss: 6.6282\n",
            "[Epoch 04] Batch 829/829 | Train Loss: 8.6446\n",
            "ðŸŸ¢ Epoch 04 | Avg Train Loss: 8.1995 | Val Loss: 5.1344\n",
            "ðŸ’¾ Checkpoint saved at epoch 5: models/checkpoint_epoch_05_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 05] Batch 001/829 | Train Loss: 8.4854\n",
            "[Epoch 05] Batch 011/829 | Train Loss: 7.2437\n",
            "[Epoch 05] Batch 021/829 | Train Loss: 5.1029\n",
            "[Epoch 05] Batch 031/829 | Train Loss: 13.9541\n",
            "[Epoch 05] Batch 041/829 | Train Loss: 8.0787\n",
            "[Epoch 05] Batch 051/829 | Train Loss: 6.9341\n",
            "[Epoch 05] Batch 061/829 | Train Loss: 7.2888\n",
            "[Epoch 05] Batch 071/829 | Train Loss: 8.4782\n",
            "[Epoch 05] Batch 081/829 | Train Loss: 7.8444\n",
            "[Epoch 05] Batch 091/829 | Train Loss: 8.1403\n",
            "[Epoch 05] Batch 101/829 | Train Loss: 6.6135\n",
            "[Epoch 05] Batch 111/829 | Train Loss: 6.4845\n",
            "[Epoch 05] Batch 121/829 | Train Loss: 10.2675\n",
            "[Epoch 05] Batch 131/829 | Train Loss: 7.2316\n",
            "[Epoch 05] Batch 141/829 | Train Loss: 9.4191\n",
            "[Epoch 05] Batch 151/829 | Train Loss: 9.3054\n",
            "[Epoch 05] Batch 161/829 | Train Loss: 10.0602\n",
            "[Epoch 05] Batch 171/829 | Train Loss: 6.9994\n",
            "[Epoch 05] Batch 181/829 | Train Loss: 7.2902\n",
            "[Epoch 05] Batch 191/829 | Train Loss: 10.1262\n",
            "[Epoch 05] Batch 201/829 | Train Loss: 7.9176\n",
            "[Epoch 05] Batch 211/829 | Train Loss: 7.4480\n",
            "[Epoch 05] Batch 221/829 | Train Loss: 8.6215\n",
            "[Epoch 05] Batch 231/829 | Train Loss: 7.5442\n",
            "[Epoch 05] Batch 241/829 | Train Loss: 10.1383\n",
            "[Epoch 05] Batch 251/829 | Train Loss: 10.3310\n",
            "[Epoch 05] Batch 261/829 | Train Loss: 6.8527\n",
            "[Epoch 05] Batch 271/829 | Train Loss: 7.9669\n",
            "[Epoch 05] Batch 281/829 | Train Loss: 8.1978\n",
            "[Epoch 05] Batch 291/829 | Train Loss: 8.4783\n",
            "[Epoch 05] Batch 301/829 | Train Loss: 8.2758\n",
            "[Epoch 05] Batch 311/829 | Train Loss: 8.6986\n",
            "[Epoch 05] Batch 321/829 | Train Loss: 9.6428\n",
            "[Epoch 05] Batch 331/829 | Train Loss: 7.3636\n",
            "[Epoch 05] Batch 341/829 | Train Loss: 7.8008\n",
            "[Epoch 05] Batch 351/829 | Train Loss: 8.0622\n",
            "[Epoch 05] Batch 361/829 | Train Loss: 7.7784\n",
            "[Epoch 05] Batch 371/829 | Train Loss: 5.9047\n",
            "[Epoch 05] Batch 381/829 | Train Loss: 9.7824\n",
            "[Epoch 05] Batch 391/829 | Train Loss: 7.8097\n",
            "[Epoch 05] Batch 401/829 | Train Loss: 9.6710\n",
            "[Epoch 05] Batch 411/829 | Train Loss: 7.1349\n",
            "[Epoch 05] Batch 421/829 | Train Loss: 8.9824\n",
            "[Epoch 05] Batch 431/829 | Train Loss: 6.9949\n",
            "[Epoch 05] Batch 441/829 | Train Loss: 6.4377\n",
            "[Epoch 05] Batch 451/829 | Train Loss: 5.5260\n",
            "[Epoch 05] Batch 461/829 | Train Loss: 6.5919\n",
            "[Epoch 05] Batch 471/829 | Train Loss: 9.3587\n",
            "[Epoch 05] Batch 481/829 | Train Loss: 6.4822\n",
            "[Epoch 05] Batch 491/829 | Train Loss: 7.0034\n",
            "[Epoch 05] Batch 501/829 | Train Loss: 6.6373\n",
            "[Epoch 05] Batch 511/829 | Train Loss: 7.8087\n",
            "[Epoch 05] Batch 521/829 | Train Loss: 9.5794\n",
            "[Epoch 05] Batch 531/829 | Train Loss: 12.6635\n",
            "[Epoch 05] Batch 541/829 | Train Loss: 7.7051\n",
            "[Epoch 05] Batch 551/829 | Train Loss: 8.0221\n",
            "[Epoch 05] Batch 561/829 | Train Loss: 8.3233\n",
            "[Epoch 05] Batch 571/829 | Train Loss: 8.7269\n",
            "[Epoch 05] Batch 581/829 | Train Loss: 6.8563\n",
            "[Epoch 05] Batch 591/829 | Train Loss: 7.0353\n",
            "[Epoch 05] Batch 601/829 | Train Loss: 6.0268\n",
            "[Epoch 05] Batch 611/829 | Train Loss: 8.7425\n",
            "[Epoch 05] Batch 621/829 | Train Loss: 10.4990\n",
            "[Epoch 05] Batch 631/829 | Train Loss: 7.2620\n",
            "[Epoch 05] Batch 641/829 | Train Loss: 7.5156\n",
            "[Epoch 05] Batch 651/829 | Train Loss: 7.5595\n",
            "[Epoch 05] Batch 661/829 | Train Loss: 9.9713\n",
            "[Epoch 05] Batch 671/829 | Train Loss: 8.4787\n",
            "[Epoch 05] Batch 681/829 | Train Loss: 8.1679\n",
            "[Epoch 05] Batch 691/829 | Train Loss: 11.6371\n",
            "[Epoch 05] Batch 701/829 | Train Loss: 7.8554\n",
            "[Epoch 05] Batch 711/829 | Train Loss: 11.0851\n",
            "[Epoch 05] Batch 721/829 | Train Loss: 7.2955\n",
            "[Epoch 05] Batch 731/829 | Train Loss: 7.7121\n",
            "[Epoch 05] Batch 741/829 | Train Loss: 6.9050\n",
            "[Epoch 05] Batch 751/829 | Train Loss: 10.3707\n",
            "[Epoch 05] Batch 761/829 | Train Loss: 9.0485\n",
            "[Epoch 05] Batch 771/829 | Train Loss: 7.8470\n",
            "[Epoch 05] Batch 781/829 | Train Loss: 9.7512\n",
            "[Epoch 05] Batch 791/829 | Train Loss: 9.2030\n",
            "[Epoch 05] Batch 801/829 | Train Loss: 11.0011\n",
            "[Epoch 05] Batch 811/829 | Train Loss: 8.9586\n",
            "[Epoch 05] Batch 821/829 | Train Loss: 7.8103\n",
            "[Epoch 05] Batch 829/829 | Train Loss: 9.1363\n",
            "ðŸŸ¢ Epoch 05 | Avg Train Loss: 8.1596 | Val Loss: 4.5890\n",
            "ðŸ’¾ Checkpoint saved at epoch 6: models/checkpoint_epoch_06_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 06] Batch 001/829 | Train Loss: 12.8023\n",
            "[Epoch 06] Batch 011/829 | Train Loss: 10.0638\n",
            "[Epoch 06] Batch 021/829 | Train Loss: 6.1981\n",
            "[Epoch 06] Batch 031/829 | Train Loss: 7.9236\n",
            "[Epoch 06] Batch 041/829 | Train Loss: 7.2644\n",
            "[Epoch 06] Batch 051/829 | Train Loss: 7.9278\n",
            "[Epoch 06] Batch 061/829 | Train Loss: 7.6936\n",
            "[Epoch 06] Batch 071/829 | Train Loss: 7.6952\n",
            "[Epoch 06] Batch 081/829 | Train Loss: 8.9529\n",
            "[Epoch 06] Batch 091/829 | Train Loss: 7.6259\n",
            "[Epoch 06] Batch 101/829 | Train Loss: 6.4665\n",
            "[Epoch 06] Batch 111/829 | Train Loss: 6.8340\n",
            "[Epoch 06] Batch 121/829 | Train Loss: 11.2884\n",
            "[Epoch 06] Batch 131/829 | Train Loss: 6.5547\n",
            "[Epoch 06] Batch 141/829 | Train Loss: 7.9078\n",
            "[Epoch 06] Batch 151/829 | Train Loss: 5.8839\n",
            "[Epoch 06] Batch 161/829 | Train Loss: 8.6277\n",
            "[Epoch 06] Batch 171/829 | Train Loss: 7.3692\n",
            "[Epoch 06] Batch 181/829 | Train Loss: 9.3630\n",
            "[Epoch 06] Batch 191/829 | Train Loss: 6.7413\n",
            "[Epoch 06] Batch 201/829 | Train Loss: 7.2702\n",
            "[Epoch 06] Batch 211/829 | Train Loss: 9.1233\n",
            "[Epoch 06] Batch 221/829 | Train Loss: 5.7990\n",
            "[Epoch 06] Batch 231/829 | Train Loss: 15.0689\n",
            "[Epoch 06] Batch 241/829 | Train Loss: 8.0588\n",
            "[Epoch 06] Batch 251/829 | Train Loss: 8.3583\n",
            "[Epoch 06] Batch 261/829 | Train Loss: 8.1725\n",
            "[Epoch 06] Batch 271/829 | Train Loss: 7.8604\n",
            "[Epoch 06] Batch 281/829 | Train Loss: 7.4974\n",
            "[Epoch 06] Batch 291/829 | Train Loss: 7.1457\n",
            "[Epoch 06] Batch 301/829 | Train Loss: 6.9659\n",
            "[Epoch 06] Batch 311/829 | Train Loss: 6.8430\n",
            "[Epoch 06] Batch 321/829 | Train Loss: 8.2996\n",
            "[Epoch 06] Batch 331/829 | Train Loss: 6.9529\n",
            "[Epoch 06] Batch 341/829 | Train Loss: 8.4759\n",
            "[Epoch 06] Batch 351/829 | Train Loss: 8.9326\n",
            "[Epoch 06] Batch 361/829 | Train Loss: 7.3129\n",
            "[Epoch 06] Batch 371/829 | Train Loss: 10.5287\n",
            "[Epoch 06] Batch 381/829 | Train Loss: 8.5242\n",
            "[Epoch 06] Batch 391/829 | Train Loss: 8.6314\n",
            "[Epoch 06] Batch 401/829 | Train Loss: 6.3329\n",
            "[Epoch 06] Batch 411/829 | Train Loss: 8.5184\n",
            "[Epoch 06] Batch 421/829 | Train Loss: 8.3731\n",
            "[Epoch 06] Batch 431/829 | Train Loss: 6.6811\n",
            "[Epoch 06] Batch 441/829 | Train Loss: 6.3570\n",
            "[Epoch 06] Batch 451/829 | Train Loss: 7.1403\n",
            "[Epoch 06] Batch 461/829 | Train Loss: 7.9129\n",
            "[Epoch 06] Batch 471/829 | Train Loss: 8.4769\n",
            "[Epoch 06] Batch 481/829 | Train Loss: 6.4561\n",
            "[Epoch 06] Batch 491/829 | Train Loss: 6.5447\n",
            "[Epoch 06] Batch 501/829 | Train Loss: 5.1770\n",
            "[Epoch 06] Batch 511/829 | Train Loss: 7.5418\n",
            "[Epoch 06] Batch 521/829 | Train Loss: 9.8389\n",
            "[Epoch 06] Batch 531/829 | Train Loss: 8.6605\n",
            "[Epoch 06] Batch 541/829 | Train Loss: 10.6964\n",
            "[Epoch 06] Batch 551/829 | Train Loss: 6.4065\n",
            "[Epoch 06] Batch 561/829 | Train Loss: 7.8219\n",
            "[Epoch 06] Batch 571/829 | Train Loss: 10.4574\n",
            "[Epoch 06] Batch 581/829 | Train Loss: 7.7962\n",
            "[Epoch 06] Batch 591/829 | Train Loss: 7.5710\n",
            "[Epoch 06] Batch 601/829 | Train Loss: 6.4781\n",
            "[Epoch 06] Batch 611/829 | Train Loss: 7.4523\n",
            "[Epoch 06] Batch 621/829 | Train Loss: 6.8661\n",
            "[Epoch 06] Batch 631/829 | Train Loss: 7.6025\n",
            "[Epoch 06] Batch 641/829 | Train Loss: 7.9560\n",
            "[Epoch 06] Batch 651/829 | Train Loss: 9.4230\n",
            "[Epoch 06] Batch 661/829 | Train Loss: 7.1496\n",
            "[Epoch 06] Batch 671/829 | Train Loss: 5.5628\n",
            "[Epoch 06] Batch 681/829 | Train Loss: 6.2264\n",
            "[Epoch 06] Batch 691/829 | Train Loss: 7.6246\n",
            "[Epoch 06] Batch 701/829 | Train Loss: 7.3145\n",
            "[Epoch 06] Batch 711/829 | Train Loss: 9.0265\n",
            "[Epoch 06] Batch 721/829 | Train Loss: 7.2564\n",
            "[Epoch 06] Batch 731/829 | Train Loss: 7.3709\n",
            "[Epoch 06] Batch 741/829 | Train Loss: 8.1284\n",
            "[Epoch 06] Batch 751/829 | Train Loss: 8.5398\n",
            "[Epoch 06] Batch 761/829 | Train Loss: 7.4794\n",
            "[Epoch 06] Batch 771/829 | Train Loss: 8.0460\n",
            "[Epoch 06] Batch 781/829 | Train Loss: 6.2338\n",
            "[Epoch 06] Batch 791/829 | Train Loss: 7.3055\n",
            "[Epoch 06] Batch 801/829 | Train Loss: 6.0281\n",
            "[Epoch 06] Batch 811/829 | Train Loss: 6.0132\n",
            "[Epoch 06] Batch 821/829 | Train Loss: 9.1888\n",
            "[Epoch 06] Batch 829/829 | Train Loss: 8.0276\n",
            "ðŸŸ¢ Epoch 06 | Avg Train Loss: 8.0241 | Val Loss: 4.9057\n",
            "ðŸ’¾ Checkpoint saved at epoch 7: models/checkpoint_epoch_07_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 07] Batch 001/829 | Train Loss: 7.3122\n",
            "[Epoch 07] Batch 011/829 | Train Loss: 8.2303\n",
            "[Epoch 07] Batch 021/829 | Train Loss: 10.6416\n",
            "[Epoch 07] Batch 031/829 | Train Loss: 6.7631\n",
            "[Epoch 07] Batch 041/829 | Train Loss: 9.1952\n",
            "[Epoch 07] Batch 051/829 | Train Loss: 4.6497\n",
            "[Epoch 07] Batch 061/829 | Train Loss: 7.0581\n",
            "[Epoch 07] Batch 071/829 | Train Loss: 8.9371\n",
            "[Epoch 07] Batch 081/829 | Train Loss: 8.6161\n",
            "[Epoch 07] Batch 091/829 | Train Loss: 8.6909\n",
            "[Epoch 07] Batch 101/829 | Train Loss: 7.2797\n",
            "[Epoch 07] Batch 111/829 | Train Loss: 8.0850\n",
            "[Epoch 07] Batch 121/829 | Train Loss: 8.6660\n",
            "[Epoch 07] Batch 131/829 | Train Loss: 6.5617\n",
            "[Epoch 07] Batch 141/829 | Train Loss: 8.4440\n",
            "[Epoch 07] Batch 151/829 | Train Loss: 7.7384\n",
            "[Epoch 07] Batch 161/829 | Train Loss: 6.6075\n",
            "[Epoch 07] Batch 171/829 | Train Loss: 7.2789\n",
            "[Epoch 07] Batch 181/829 | Train Loss: 8.4071\n",
            "[Epoch 07] Batch 191/829 | Train Loss: 6.7904\n",
            "[Epoch 07] Batch 201/829 | Train Loss: 9.0243\n",
            "[Epoch 07] Batch 211/829 | Train Loss: 8.3566\n",
            "[Epoch 07] Batch 221/829 | Train Loss: 8.6461\n",
            "[Epoch 07] Batch 231/829 | Train Loss: 9.4698\n",
            "[Epoch 07] Batch 241/829 | Train Loss: 8.2034\n",
            "[Epoch 07] Batch 251/829 | Train Loss: 8.5026\n",
            "[Epoch 07] Batch 261/829 | Train Loss: 6.7718\n",
            "[Epoch 07] Batch 271/829 | Train Loss: 7.9950\n",
            "[Epoch 07] Batch 281/829 | Train Loss: 6.0301\n",
            "[Epoch 07] Batch 291/829 | Train Loss: 7.9535\n",
            "[Epoch 07] Batch 301/829 | Train Loss: 5.9712\n",
            "[Epoch 07] Batch 311/829 | Train Loss: 7.6435\n",
            "[Epoch 07] Batch 321/829 | Train Loss: 5.1195\n",
            "[Epoch 07] Batch 331/829 | Train Loss: 8.6886\n",
            "[Epoch 07] Batch 341/829 | Train Loss: 6.9721\n",
            "[Epoch 07] Batch 351/829 | Train Loss: 7.3674\n",
            "[Epoch 07] Batch 361/829 | Train Loss: 8.1558\n",
            "[Epoch 07] Batch 371/829 | Train Loss: 8.9571\n",
            "[Epoch 07] Batch 381/829 | Train Loss: 6.4491\n",
            "[Epoch 07] Batch 391/829 | Train Loss: 6.2907\n",
            "[Epoch 07] Batch 401/829 | Train Loss: 11.4120\n",
            "[Epoch 07] Batch 411/829 | Train Loss: 9.8389\n",
            "[Epoch 07] Batch 421/829 | Train Loss: 7.6078\n",
            "[Epoch 07] Batch 431/829 | Train Loss: 8.3248\n",
            "[Epoch 07] Batch 441/829 | Train Loss: 7.0332\n",
            "[Epoch 07] Batch 451/829 | Train Loss: 6.1513\n",
            "[Epoch 07] Batch 461/829 | Train Loss: 8.1449\n",
            "[Epoch 07] Batch 471/829 | Train Loss: 6.4559\n",
            "[Epoch 07] Batch 481/829 | Train Loss: 7.0722\n",
            "[Epoch 07] Batch 491/829 | Train Loss: 9.9829\n",
            "[Epoch 07] Batch 501/829 | Train Loss: 8.3576\n",
            "[Epoch 07] Batch 511/829 | Train Loss: 8.3260\n",
            "[Epoch 07] Batch 521/829 | Train Loss: 8.0242\n",
            "[Epoch 07] Batch 531/829 | Train Loss: 6.8779\n",
            "[Epoch 07] Batch 541/829 | Train Loss: 6.7422\n",
            "[Epoch 07] Batch 551/829 | Train Loss: 8.7171\n",
            "[Epoch 07] Batch 561/829 | Train Loss: 12.2705\n",
            "[Epoch 07] Batch 571/829 | Train Loss: 7.3406\n",
            "[Epoch 07] Batch 581/829 | Train Loss: 7.6865\n",
            "[Epoch 07] Batch 591/829 | Train Loss: 9.1809\n",
            "[Epoch 07] Batch 601/829 | Train Loss: 6.4920\n",
            "[Epoch 07] Batch 611/829 | Train Loss: 7.6526\n",
            "[Epoch 07] Batch 621/829 | Train Loss: 8.7215\n",
            "[Epoch 07] Batch 631/829 | Train Loss: 8.2429\n",
            "[Epoch 07] Batch 641/829 | Train Loss: 9.9077\n",
            "[Epoch 07] Batch 651/829 | Train Loss: 7.9649\n",
            "[Epoch 07] Batch 661/829 | Train Loss: 5.6707\n",
            "[Epoch 07] Batch 671/829 | Train Loss: 6.0510\n",
            "[Epoch 07] Batch 681/829 | Train Loss: 6.1333\n",
            "[Epoch 07] Batch 691/829 | Train Loss: 8.3689\n",
            "[Epoch 07] Batch 701/829 | Train Loss: 6.9419\n",
            "[Epoch 07] Batch 711/829 | Train Loss: 9.7433\n",
            "[Epoch 07] Batch 721/829 | Train Loss: 6.6528\n",
            "[Epoch 07] Batch 731/829 | Train Loss: 6.0399\n",
            "[Epoch 07] Batch 741/829 | Train Loss: 6.0131\n",
            "[Epoch 07] Batch 751/829 | Train Loss: 8.3758\n",
            "[Epoch 07] Batch 761/829 | Train Loss: 7.2137\n",
            "[Epoch 07] Batch 771/829 | Train Loss: 7.2903\n",
            "[Epoch 07] Batch 781/829 | Train Loss: 9.1474\n",
            "[Epoch 07] Batch 791/829 | Train Loss: 4.6458\n",
            "[Epoch 07] Batch 801/829 | Train Loss: 8.0124\n",
            "[Epoch 07] Batch 811/829 | Train Loss: 6.1862\n",
            "[Epoch 07] Batch 821/829 | Train Loss: 8.5573\n",
            "[Epoch 07] Batch 829/829 | Train Loss: 8.1611\n",
            "ðŸŸ¢ Epoch 07 | Avg Train Loss: 7.8647 | Val Loss: 4.6073\n",
            "ðŸ’¾ Checkpoint saved at epoch 8: models/checkpoint_epoch_08_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 08] Batch 001/829 | Train Loss: 6.0285\n",
            "[Epoch 08] Batch 011/829 | Train Loss: 8.5202\n",
            "[Epoch 08] Batch 021/829 | Train Loss: 7.2982\n",
            "[Epoch 08] Batch 031/829 | Train Loss: 9.9673\n",
            "[Epoch 08] Batch 041/829 | Train Loss: 8.0119\n",
            "[Epoch 08] Batch 051/829 | Train Loss: 7.0204\n",
            "[Epoch 08] Batch 061/829 | Train Loss: 5.3392\n",
            "[Epoch 08] Batch 071/829 | Train Loss: 10.9239\n",
            "[Epoch 08] Batch 081/829 | Train Loss: 9.4495\n",
            "[Epoch 08] Batch 091/829 | Train Loss: 6.2657\n",
            "[Epoch 08] Batch 101/829 | Train Loss: 8.1832\n",
            "[Epoch 08] Batch 111/829 | Train Loss: 8.4480\n",
            "[Epoch 08] Batch 121/829 | Train Loss: 7.5934\n",
            "[Epoch 08] Batch 131/829 | Train Loss: 5.7351\n",
            "[Epoch 08] Batch 141/829 | Train Loss: 11.3596\n",
            "[Epoch 08] Batch 151/829 | Train Loss: 6.7156\n",
            "[Epoch 08] Batch 161/829 | Train Loss: 10.8586\n",
            "[Epoch 08] Batch 171/829 | Train Loss: 8.7195\n",
            "[Epoch 08] Batch 181/829 | Train Loss: 5.0883\n",
            "[Epoch 08] Batch 191/829 | Train Loss: 5.4404\n",
            "[Epoch 08] Batch 201/829 | Train Loss: 8.3266\n",
            "[Epoch 08] Batch 211/829 | Train Loss: 5.9541\n",
            "[Epoch 08] Batch 221/829 | Train Loss: 10.8162\n",
            "[Epoch 08] Batch 231/829 | Train Loss: 8.2332\n",
            "[Epoch 08] Batch 241/829 | Train Loss: 6.0846\n",
            "[Epoch 08] Batch 251/829 | Train Loss: 8.0439\n",
            "[Epoch 08] Batch 261/829 | Train Loss: 8.0785\n",
            "[Epoch 08] Batch 271/829 | Train Loss: 8.4264\n",
            "[Epoch 08] Batch 281/829 | Train Loss: 5.7944\n",
            "[Epoch 08] Batch 291/829 | Train Loss: 9.6126\n",
            "[Epoch 08] Batch 301/829 | Train Loss: 8.6756\n",
            "[Epoch 08] Batch 311/829 | Train Loss: 10.5050\n",
            "[Epoch 08] Batch 321/829 | Train Loss: 11.1761\n",
            "[Epoch 08] Batch 331/829 | Train Loss: 8.2525\n",
            "[Epoch 08] Batch 341/829 | Train Loss: 8.7523\n",
            "[Epoch 08] Batch 351/829 | Train Loss: 9.7339\n",
            "[Epoch 08] Batch 361/829 | Train Loss: 10.5962\n",
            "[Epoch 08] Batch 371/829 | Train Loss: 7.5679\n",
            "[Epoch 08] Batch 381/829 | Train Loss: 8.3984\n",
            "[Epoch 08] Batch 391/829 | Train Loss: 6.1035\n",
            "[Epoch 08] Batch 401/829 | Train Loss: 9.7042\n",
            "[Epoch 08] Batch 411/829 | Train Loss: 7.3498\n",
            "[Epoch 08] Batch 421/829 | Train Loss: 10.7163\n",
            "[Epoch 08] Batch 431/829 | Train Loss: 7.1672\n",
            "[Epoch 08] Batch 441/829 | Train Loss: 7.7112\n",
            "[Epoch 08] Batch 451/829 | Train Loss: 6.0274\n",
            "[Epoch 08] Batch 461/829 | Train Loss: 8.3332\n",
            "[Epoch 08] Batch 471/829 | Train Loss: 7.5375\n",
            "[Epoch 08] Batch 481/829 | Train Loss: 6.3582\n",
            "[Epoch 08] Batch 491/829 | Train Loss: 7.4649\n",
            "[Epoch 08] Batch 501/829 | Train Loss: 7.7755\n",
            "[Epoch 08] Batch 511/829 | Train Loss: 8.4961\n",
            "[Epoch 08] Batch 521/829 | Train Loss: 6.6519\n",
            "[Epoch 08] Batch 531/829 | Train Loss: 8.6032\n",
            "[Epoch 08] Batch 541/829 | Train Loss: 5.8097\n",
            "[Epoch 08] Batch 551/829 | Train Loss: 8.0294\n",
            "[Epoch 08] Batch 561/829 | Train Loss: 10.3307\n",
            "[Epoch 08] Batch 571/829 | Train Loss: 5.1349\n",
            "[Epoch 08] Batch 581/829 | Train Loss: 7.7759\n",
            "[Epoch 08] Batch 591/829 | Train Loss: 8.6584\n",
            "[Epoch 08] Batch 601/829 | Train Loss: 10.0225\n",
            "[Epoch 08] Batch 611/829 | Train Loss: 7.8068\n",
            "[Epoch 08] Batch 621/829 | Train Loss: 10.9821\n",
            "[Epoch 08] Batch 631/829 | Train Loss: 8.3081\n",
            "[Epoch 08] Batch 641/829 | Train Loss: 8.2492\n",
            "[Epoch 08] Batch 651/829 | Train Loss: 8.5627\n",
            "[Epoch 08] Batch 661/829 | Train Loss: 6.6617\n",
            "[Epoch 08] Batch 671/829 | Train Loss: 8.5727\n",
            "[Epoch 08] Batch 681/829 | Train Loss: 10.3250\n",
            "[Epoch 08] Batch 691/829 | Train Loss: 7.4110\n",
            "[Epoch 08] Batch 701/829 | Train Loss: 6.7240\n",
            "[Epoch 08] Batch 711/829 | Train Loss: 6.6775\n",
            "[Epoch 08] Batch 721/829 | Train Loss: 11.8018\n",
            "[Epoch 08] Batch 731/829 | Train Loss: 10.0384\n",
            "[Epoch 08] Batch 741/829 | Train Loss: 7.7589\n",
            "[Epoch 08] Batch 751/829 | Train Loss: 8.0106\n",
            "[Epoch 08] Batch 761/829 | Train Loss: 8.0666\n",
            "[Epoch 08] Batch 771/829 | Train Loss: 9.4128\n",
            "[Epoch 08] Batch 781/829 | Train Loss: 7.7090\n",
            "[Epoch 08] Batch 791/829 | Train Loss: 9.2391\n",
            "[Epoch 08] Batch 801/829 | Train Loss: 9.3487\n",
            "[Epoch 08] Batch 811/829 | Train Loss: 8.4434\n",
            "[Epoch 08] Batch 821/829 | Train Loss: 9.4874\n",
            "[Epoch 08] Batch 829/829 | Train Loss: 10.1329\n",
            "ðŸŸ¢ Epoch 08 | Avg Train Loss: 7.8388 | Val Loss: 7.3464\n",
            "ðŸ’¾ Checkpoint saved at epoch 9: models/checkpoint_epoch_09_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 09] Batch 001/829 | Train Loss: 12.5973\n",
            "[Epoch 09] Batch 011/829 | Train Loss: 7.5885\n",
            "[Epoch 09] Batch 021/829 | Train Loss: 4.7503\n",
            "[Epoch 09] Batch 031/829 | Train Loss: 8.7910\n",
            "[Epoch 09] Batch 041/829 | Train Loss: 6.4088\n",
            "[Epoch 09] Batch 051/829 | Train Loss: 8.4734\n",
            "[Epoch 09] Batch 061/829 | Train Loss: 8.2297\n",
            "[Epoch 09] Batch 071/829 | Train Loss: 8.7472\n",
            "[Epoch 09] Batch 081/829 | Train Loss: 5.3034\n",
            "[Epoch 09] Batch 091/829 | Train Loss: 6.6612\n",
            "[Epoch 09] Batch 101/829 | Train Loss: 5.9491\n",
            "[Epoch 09] Batch 111/829 | Train Loss: 7.5964\n",
            "[Epoch 09] Batch 121/829 | Train Loss: 6.1277\n",
            "[Epoch 09] Batch 131/829 | Train Loss: 7.5252\n",
            "[Epoch 09] Batch 141/829 | Train Loss: 6.5062\n",
            "[Epoch 09] Batch 151/829 | Train Loss: 9.4505\n",
            "[Epoch 09] Batch 161/829 | Train Loss: 9.5909\n",
            "[Epoch 09] Batch 171/829 | Train Loss: 10.2095\n",
            "[Epoch 09] Batch 181/829 | Train Loss: 8.8533\n",
            "[Epoch 09] Batch 191/829 | Train Loss: 7.0887\n",
            "[Epoch 09] Batch 201/829 | Train Loss: 7.4596\n",
            "[Epoch 09] Batch 211/829 | Train Loss: 5.8702\n",
            "[Epoch 09] Batch 221/829 | Train Loss: 6.8682\n",
            "[Epoch 09] Batch 231/829 | Train Loss: 9.7626\n",
            "[Epoch 09] Batch 241/829 | Train Loss: 7.4807\n",
            "[Epoch 09] Batch 251/829 | Train Loss: 6.0109\n",
            "[Epoch 09] Batch 261/829 | Train Loss: 8.0807\n",
            "[Epoch 09] Batch 271/829 | Train Loss: 9.5136\n",
            "[Epoch 09] Batch 281/829 | Train Loss: 5.7731\n",
            "[Epoch 09] Batch 291/829 | Train Loss: 6.3387\n",
            "[Epoch 09] Batch 301/829 | Train Loss: 7.5554\n",
            "[Epoch 09] Batch 311/829 | Train Loss: 6.9518\n",
            "[Epoch 09] Batch 321/829 | Train Loss: 7.7483\n",
            "[Epoch 09] Batch 331/829 | Train Loss: 5.9681\n",
            "[Epoch 09] Batch 341/829 | Train Loss: 6.4451\n",
            "[Epoch 09] Batch 351/829 | Train Loss: 6.8270\n",
            "[Epoch 09] Batch 361/829 | Train Loss: 6.9314\n",
            "[Epoch 09] Batch 371/829 | Train Loss: 7.2337\n",
            "[Epoch 09] Batch 381/829 | Train Loss: 8.9699\n",
            "[Epoch 09] Batch 391/829 | Train Loss: 6.5419\n",
            "[Epoch 09] Batch 401/829 | Train Loss: 8.4367\n",
            "[Epoch 09] Batch 411/829 | Train Loss: 8.5066\n",
            "[Epoch 09] Batch 421/829 | Train Loss: 7.0067\n",
            "[Epoch 09] Batch 431/829 | Train Loss: 6.4138\n",
            "[Epoch 09] Batch 441/829 | Train Loss: 6.5092\n",
            "[Epoch 09] Batch 451/829 | Train Loss: 7.2691\n",
            "[Epoch 09] Batch 461/829 | Train Loss: 8.1943\n",
            "[Epoch 09] Batch 471/829 | Train Loss: 12.3503\n",
            "[Epoch 09] Batch 481/829 | Train Loss: 4.6941\n",
            "[Epoch 09] Batch 491/829 | Train Loss: 6.6583\n",
            "[Epoch 09] Batch 501/829 | Train Loss: 6.3837\n",
            "[Epoch 09] Batch 511/829 | Train Loss: 8.4403\n",
            "[Epoch 09] Batch 521/829 | Train Loss: 5.8430\n",
            "[Epoch 09] Batch 531/829 | Train Loss: 7.0899\n",
            "[Epoch 09] Batch 541/829 | Train Loss: 8.4456\n",
            "[Epoch 09] Batch 551/829 | Train Loss: 7.7086\n",
            "[Epoch 09] Batch 561/829 | Train Loss: 7.4036\n",
            "[Epoch 09] Batch 571/829 | Train Loss: 8.0421\n",
            "[Epoch 09] Batch 581/829 | Train Loss: 6.1449\n",
            "[Epoch 09] Batch 591/829 | Train Loss: 6.9363\n",
            "[Epoch 09] Batch 601/829 | Train Loss: 6.8589\n",
            "[Epoch 09] Batch 611/829 | Train Loss: 8.0082\n",
            "[Epoch 09] Batch 621/829 | Train Loss: 4.8788\n",
            "[Epoch 09] Batch 631/829 | Train Loss: 6.9714\n",
            "[Epoch 09] Batch 641/829 | Train Loss: 6.7260\n",
            "[Epoch 09] Batch 651/829 | Train Loss: 6.1798\n",
            "[Epoch 09] Batch 661/829 | Train Loss: 8.1306\n",
            "[Epoch 09] Batch 671/829 | Train Loss: 6.7678\n",
            "[Epoch 09] Batch 681/829 | Train Loss: 8.4825\n",
            "[Epoch 09] Batch 691/829 | Train Loss: 9.1911\n",
            "[Epoch 09] Batch 701/829 | Train Loss: 9.0419\n",
            "[Epoch 09] Batch 711/829 | Train Loss: 8.4744\n",
            "[Epoch 09] Batch 721/829 | Train Loss: 7.4631\n",
            "[Epoch 09] Batch 731/829 | Train Loss: 7.3159\n",
            "[Epoch 09] Batch 741/829 | Train Loss: 10.3049\n",
            "[Epoch 09] Batch 751/829 | Train Loss: 7.2751\n",
            "[Epoch 09] Batch 761/829 | Train Loss: 5.2672\n",
            "[Epoch 09] Batch 771/829 | Train Loss: 5.7354\n",
            "[Epoch 09] Batch 781/829 | Train Loss: 6.7451\n",
            "[Epoch 09] Batch 791/829 | Train Loss: 9.6252\n",
            "[Epoch 09] Batch 801/829 | Train Loss: 6.8684\n",
            "[Epoch 09] Batch 811/829 | Train Loss: 7.3000\n",
            "[Epoch 09] Batch 821/829 | Train Loss: 12.9781\n",
            "[Epoch 09] Batch 829/829 | Train Loss: 8.4336\n",
            "ðŸŸ¢ Epoch 09 | Avg Train Loss: 7.8577 | Val Loss: 5.1359\n",
            "ðŸ’¾ Checkpoint saved at epoch 10: models/checkpoint_epoch_10_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 10] Batch 001/829 | Train Loss: 7.2491\n",
            "[Epoch 10] Batch 011/829 | Train Loss: 9.9643\n",
            "[Epoch 10] Batch 021/829 | Train Loss: 7.9439\n",
            "[Epoch 10] Batch 031/829 | Train Loss: 8.4958\n",
            "[Epoch 10] Batch 041/829 | Train Loss: 8.9845\n",
            "[Epoch 10] Batch 051/829 | Train Loss: 7.2668\n",
            "[Epoch 10] Batch 061/829 | Train Loss: 6.7506\n",
            "[Epoch 10] Batch 071/829 | Train Loss: 7.1272\n",
            "[Epoch 10] Batch 081/829 | Train Loss: 8.6833\n",
            "[Epoch 10] Batch 091/829 | Train Loss: 6.0236\n",
            "[Epoch 10] Batch 101/829 | Train Loss: 7.6245\n",
            "[Epoch 10] Batch 111/829 | Train Loss: 8.0034\n",
            "[Epoch 10] Batch 121/829 | Train Loss: 8.7597\n",
            "[Epoch 10] Batch 131/829 | Train Loss: 13.8021\n",
            "[Epoch 10] Batch 141/829 | Train Loss: 7.8607\n",
            "[Epoch 10] Batch 151/829 | Train Loss: 7.6285\n",
            "[Epoch 10] Batch 161/829 | Train Loss: 7.2537\n",
            "[Epoch 10] Batch 171/829 | Train Loss: 8.6056\n",
            "[Epoch 10] Batch 181/829 | Train Loss: 9.1234\n",
            "[Epoch 10] Batch 191/829 | Train Loss: 7.1131\n",
            "[Epoch 10] Batch 201/829 | Train Loss: 5.7195\n",
            "[Epoch 10] Batch 211/829 | Train Loss: 10.2801\n",
            "[Epoch 10] Batch 221/829 | Train Loss: 7.0958\n",
            "[Epoch 10] Batch 231/829 | Train Loss: 7.4094\n",
            "[Epoch 10] Batch 241/829 | Train Loss: 6.7557\n",
            "[Epoch 10] Batch 251/829 | Train Loss: 8.0590\n",
            "[Epoch 10] Batch 261/829 | Train Loss: 9.0526\n",
            "[Epoch 10] Batch 271/829 | Train Loss: 7.8378\n",
            "[Epoch 10] Batch 281/829 | Train Loss: 4.8163\n",
            "[Epoch 10] Batch 291/829 | Train Loss: 6.6367\n",
            "[Epoch 10] Batch 301/829 | Train Loss: 8.5025\n",
            "[Epoch 10] Batch 311/829 | Train Loss: 11.8188\n",
            "[Epoch 10] Batch 321/829 | Train Loss: 6.6616\n",
            "[Epoch 10] Batch 331/829 | Train Loss: 6.5918\n",
            "[Epoch 10] Batch 341/829 | Train Loss: 5.9300\n",
            "[Epoch 10] Batch 351/829 | Train Loss: 8.1207\n",
            "[Epoch 10] Batch 361/829 | Train Loss: 7.6045\n",
            "[Epoch 10] Batch 371/829 | Train Loss: 11.0714\n",
            "[Epoch 10] Batch 381/829 | Train Loss: 5.1138\n",
            "[Epoch 10] Batch 391/829 | Train Loss: 10.4782\n",
            "[Epoch 10] Batch 401/829 | Train Loss: 8.5579\n",
            "[Epoch 10] Batch 411/829 | Train Loss: 11.0387\n",
            "[Epoch 10] Batch 421/829 | Train Loss: 5.7962\n",
            "[Epoch 10] Batch 431/829 | Train Loss: 6.5952\n",
            "[Epoch 10] Batch 441/829 | Train Loss: 7.3766\n",
            "[Epoch 10] Batch 451/829 | Train Loss: 8.9648\n",
            "[Epoch 10] Batch 461/829 | Train Loss: 9.1625\n",
            "[Epoch 10] Batch 471/829 | Train Loss: 10.7720\n",
            "[Epoch 10] Batch 481/829 | Train Loss: 8.2017\n",
            "[Epoch 10] Batch 491/829 | Train Loss: 7.5368\n",
            "[Epoch 10] Batch 501/829 | Train Loss: 8.2231\n",
            "[Epoch 10] Batch 511/829 | Train Loss: 8.6129\n",
            "[Epoch 10] Batch 521/829 | Train Loss: 9.4573\n",
            "[Epoch 10] Batch 531/829 | Train Loss: 6.5057\n",
            "[Epoch 10] Batch 541/829 | Train Loss: 6.9897\n",
            "[Epoch 10] Batch 551/829 | Train Loss: 7.3852\n",
            "[Epoch 10] Batch 561/829 | Train Loss: 6.2495\n",
            "[Epoch 10] Batch 571/829 | Train Loss: 7.8170\n",
            "[Epoch 10] Batch 581/829 | Train Loss: 6.8880\n",
            "[Epoch 10] Batch 591/829 | Train Loss: 8.7950\n",
            "[Epoch 10] Batch 601/829 | Train Loss: 7.7270\n",
            "[Epoch 10] Batch 611/829 | Train Loss: 5.0938\n",
            "[Epoch 10] Batch 621/829 | Train Loss: 8.1073\n",
            "[Epoch 10] Batch 631/829 | Train Loss: 8.3776\n",
            "[Epoch 10] Batch 641/829 | Train Loss: 6.4224\n",
            "[Epoch 10] Batch 651/829 | Train Loss: 7.2028\n",
            "[Epoch 10] Batch 661/829 | Train Loss: 9.3674\n",
            "[Epoch 10] Batch 671/829 | Train Loss: 8.7066\n",
            "[Epoch 10] Batch 681/829 | Train Loss: 8.2666\n",
            "[Epoch 10] Batch 691/829 | Train Loss: 7.0618\n",
            "[Epoch 10] Batch 701/829 | Train Loss: 9.2180\n",
            "[Epoch 10] Batch 711/829 | Train Loss: 4.2804\n",
            "[Epoch 10] Batch 721/829 | Train Loss: 7.0693\n",
            "[Epoch 10] Batch 731/829 | Train Loss: 6.7690\n",
            "[Epoch 10] Batch 741/829 | Train Loss: 7.8928\n",
            "[Epoch 10] Batch 751/829 | Train Loss: 9.7432\n",
            "[Epoch 10] Batch 761/829 | Train Loss: 5.6359\n",
            "[Epoch 10] Batch 771/829 | Train Loss: 8.4210\n",
            "[Epoch 10] Batch 781/829 | Train Loss: 7.1375\n",
            "[Epoch 10] Batch 791/829 | Train Loss: 8.2447\n",
            "[Epoch 10] Batch 801/829 | Train Loss: 7.4713\n",
            "[Epoch 10] Batch 811/829 | Train Loss: 7.8174\n",
            "[Epoch 10] Batch 821/829 | Train Loss: 6.3798\n",
            "[Epoch 10] Batch 829/829 | Train Loss: 6.6553\n",
            "ðŸŸ¢ Epoch 10 | Avg Train Loss: 7.7530 | Val Loss: 4.6268\n",
            "ðŸ’¾ Checkpoint saved at epoch 11: models/checkpoint_epoch_11_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 11] Batch 001/829 | Train Loss: 11.1423\n",
            "[Epoch 11] Batch 011/829 | Train Loss: 6.1147\n",
            "[Epoch 11] Batch 021/829 | Train Loss: 6.7668\n",
            "[Epoch 11] Batch 031/829 | Train Loss: 9.7563\n",
            "[Epoch 11] Batch 041/829 | Train Loss: 6.9443\n",
            "[Epoch 11] Batch 051/829 | Train Loss: 6.8531\n",
            "[Epoch 11] Batch 061/829 | Train Loss: 5.8278\n",
            "[Epoch 11] Batch 071/829 | Train Loss: 6.9221\n",
            "[Epoch 11] Batch 081/829 | Train Loss: 6.4585\n",
            "[Epoch 11] Batch 091/829 | Train Loss: 5.2747\n",
            "[Epoch 11] Batch 101/829 | Train Loss: 8.4247\n",
            "[Epoch 11] Batch 111/829 | Train Loss: 6.4419\n",
            "[Epoch 11] Batch 121/829 | Train Loss: 9.6719\n",
            "[Epoch 11] Batch 131/829 | Train Loss: 8.4892\n",
            "[Epoch 11] Batch 141/829 | Train Loss: 7.6474\n",
            "[Epoch 11] Batch 151/829 | Train Loss: 7.6502\n",
            "[Epoch 11] Batch 161/829 | Train Loss: 5.3070\n",
            "[Epoch 11] Batch 171/829 | Train Loss: 6.6128\n",
            "[Epoch 11] Batch 181/829 | Train Loss: 10.4241\n",
            "[Epoch 11] Batch 191/829 | Train Loss: 7.1009\n",
            "[Epoch 11] Batch 201/829 | Train Loss: 5.6370\n",
            "[Epoch 11] Batch 211/829 | Train Loss: 7.5441\n",
            "[Epoch 11] Batch 221/829 | Train Loss: 9.1299\n",
            "[Epoch 11] Batch 231/829 | Train Loss: 9.1711\n",
            "[Epoch 11] Batch 241/829 | Train Loss: 8.6867\n",
            "[Epoch 11] Batch 251/829 | Train Loss: 5.5137\n",
            "[Epoch 11] Batch 261/829 | Train Loss: 6.3047\n",
            "[Epoch 11] Batch 271/829 | Train Loss: 9.0305\n",
            "[Epoch 11] Batch 281/829 | Train Loss: 5.8336\n",
            "[Epoch 11] Batch 291/829 | Train Loss: 7.6734\n",
            "[Epoch 11] Batch 301/829 | Train Loss: 5.9147\n",
            "[Epoch 11] Batch 311/829 | Train Loss: 5.7726\n",
            "[Epoch 11] Batch 321/829 | Train Loss: 8.2921\n",
            "[Epoch 11] Batch 331/829 | Train Loss: 8.5583\n",
            "[Epoch 11] Batch 341/829 | Train Loss: 5.7295\n",
            "[Epoch 11] Batch 351/829 | Train Loss: 8.1677\n",
            "[Epoch 11] Batch 361/829 | Train Loss: 8.0584\n",
            "[Epoch 11] Batch 371/829 | Train Loss: 7.9279\n",
            "[Epoch 11] Batch 381/829 | Train Loss: 7.8898\n",
            "[Epoch 11] Batch 391/829 | Train Loss: 9.2921\n",
            "[Epoch 11] Batch 401/829 | Train Loss: 11.8389\n",
            "[Epoch 11] Batch 411/829 | Train Loss: 8.7156\n",
            "[Epoch 11] Batch 421/829 | Train Loss: 7.1911\n",
            "[Epoch 11] Batch 431/829 | Train Loss: 4.8946\n",
            "[Epoch 11] Batch 441/829 | Train Loss: 7.5650\n",
            "[Epoch 11] Batch 451/829 | Train Loss: 8.7288\n",
            "[Epoch 11] Batch 461/829 | Train Loss: 7.4499\n",
            "[Epoch 11] Batch 471/829 | Train Loss: 7.3379\n",
            "[Epoch 11] Batch 481/829 | Train Loss: 6.8493\n",
            "[Epoch 11] Batch 491/829 | Train Loss: 8.1852\n",
            "[Epoch 11] Batch 501/829 | Train Loss: 6.9252\n",
            "[Epoch 11] Batch 511/829 | Train Loss: 6.9135\n",
            "[Epoch 11] Batch 521/829 | Train Loss: 6.4379\n",
            "[Epoch 11] Batch 531/829 | Train Loss: 9.0401\n",
            "[Epoch 11] Batch 541/829 | Train Loss: 7.1750\n",
            "[Epoch 11] Batch 551/829 | Train Loss: 6.2664\n",
            "[Epoch 11] Batch 561/829 | Train Loss: 8.3092\n",
            "[Epoch 11] Batch 571/829 | Train Loss: 8.2221\n",
            "[Epoch 11] Batch 581/829 | Train Loss: 8.8476\n",
            "[Epoch 11] Batch 591/829 | Train Loss: 6.5701\n",
            "[Epoch 11] Batch 601/829 | Train Loss: 6.0107\n",
            "[Epoch 11] Batch 611/829 | Train Loss: 8.5462\n",
            "[Epoch 11] Batch 621/829 | Train Loss: 7.2351\n",
            "[Epoch 11] Batch 631/829 | Train Loss: 7.2660\n",
            "[Epoch 11] Batch 641/829 | Train Loss: 6.2124\n",
            "[Epoch 11] Batch 651/829 | Train Loss: 8.5182\n",
            "[Epoch 11] Batch 661/829 | Train Loss: 8.6029\n",
            "[Epoch 11] Batch 671/829 | Train Loss: 10.5317\n",
            "[Epoch 11] Batch 681/829 | Train Loss: 6.8996\n",
            "[Epoch 11] Batch 691/829 | Train Loss: 8.1927\n",
            "[Epoch 11] Batch 701/829 | Train Loss: 7.0757\n",
            "[Epoch 11] Batch 711/829 | Train Loss: 6.4074\n",
            "[Epoch 11] Batch 721/829 | Train Loss: 10.1275\n",
            "[Epoch 11] Batch 731/829 | Train Loss: 5.7295\n",
            "[Epoch 11] Batch 741/829 | Train Loss: 11.8349\n",
            "[Epoch 11] Batch 751/829 | Train Loss: 6.3247\n",
            "[Epoch 11] Batch 761/829 | Train Loss: 7.6371\n",
            "[Epoch 11] Batch 771/829 | Train Loss: 8.6238\n",
            "[Epoch 11] Batch 781/829 | Train Loss: 6.9814\n",
            "[Epoch 11] Batch 791/829 | Train Loss: 6.5405\n",
            "[Epoch 11] Batch 801/829 | Train Loss: 5.0770\n",
            "[Epoch 11] Batch 811/829 | Train Loss: 8.9181\n",
            "[Epoch 11] Batch 821/829 | Train Loss: 7.6392\n",
            "[Epoch 11] Batch 829/829 | Train Loss: 5.9783\n",
            "ðŸŸ¢ Epoch 11 | Avg Train Loss: 7.7807 | Val Loss: 5.3326\n",
            "ðŸ’¾ Checkpoint saved at epoch 12: models/checkpoint_epoch_12_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 12] Batch 001/829 | Train Loss: 6.2319\n",
            "[Epoch 12] Batch 011/829 | Train Loss: 6.7832\n",
            "[Epoch 12] Batch 021/829 | Train Loss: 8.6055\n",
            "[Epoch 12] Batch 031/829 | Train Loss: 5.4035\n",
            "[Epoch 12] Batch 041/829 | Train Loss: 7.9965\n",
            "[Epoch 12] Batch 051/829 | Train Loss: 7.6056\n",
            "[Epoch 12] Batch 061/829 | Train Loss: 9.7599\n",
            "[Epoch 12] Batch 071/829 | Train Loss: 8.6701\n",
            "[Epoch 12] Batch 081/829 | Train Loss: 6.6308\n",
            "[Epoch 12] Batch 091/829 | Train Loss: 8.6587\n",
            "[Epoch 12] Batch 101/829 | Train Loss: 8.2201\n",
            "[Epoch 12] Batch 111/829 | Train Loss: 8.5742\n",
            "[Epoch 12] Batch 121/829 | Train Loss: 7.9127\n",
            "[Epoch 12] Batch 131/829 | Train Loss: 7.5895\n",
            "[Epoch 12] Batch 141/829 | Train Loss: 6.7293\n",
            "[Epoch 12] Batch 151/829 | Train Loss: 8.3906\n",
            "[Epoch 12] Batch 161/829 | Train Loss: 8.0881\n",
            "[Epoch 12] Batch 171/829 | Train Loss: 8.0777\n",
            "[Epoch 12] Batch 181/829 | Train Loss: 8.1076\n",
            "[Epoch 12] Batch 191/829 | Train Loss: 7.2356\n",
            "[Epoch 12] Batch 201/829 | Train Loss: 6.5095\n",
            "[Epoch 12] Batch 211/829 | Train Loss: 7.2019\n",
            "[Epoch 12] Batch 221/829 | Train Loss: 9.1933\n",
            "[Epoch 12] Batch 231/829 | Train Loss: 8.1227\n",
            "[Epoch 12] Batch 241/829 | Train Loss: 9.3580\n",
            "[Epoch 12] Batch 251/829 | Train Loss: 5.8249\n",
            "[Epoch 12] Batch 261/829 | Train Loss: 6.1811\n",
            "[Epoch 12] Batch 271/829 | Train Loss: 8.6719\n",
            "[Epoch 12] Batch 281/829 | Train Loss: 6.9819\n",
            "[Epoch 12] Batch 291/829 | Train Loss: 8.4223\n",
            "[Epoch 12] Batch 301/829 | Train Loss: 8.8343\n",
            "[Epoch 12] Batch 311/829 | Train Loss: 6.5756\n",
            "[Epoch 12] Batch 321/829 | Train Loss: 8.9266\n",
            "[Epoch 12] Batch 331/829 | Train Loss: 7.8610\n",
            "[Epoch 12] Batch 341/829 | Train Loss: 9.3648\n",
            "[Epoch 12] Batch 351/829 | Train Loss: 7.2090\n",
            "[Epoch 12] Batch 361/829 | Train Loss: 7.0990\n",
            "[Epoch 12] Batch 371/829 | Train Loss: 7.2529\n",
            "[Epoch 12] Batch 381/829 | Train Loss: 6.4007\n",
            "[Epoch 12] Batch 391/829 | Train Loss: 6.4832\n",
            "[Epoch 12] Batch 401/829 | Train Loss: 5.2626\n",
            "[Epoch 12] Batch 411/829 | Train Loss: 8.2627\n",
            "[Epoch 12] Batch 421/829 | Train Loss: 7.0080\n",
            "[Epoch 12] Batch 431/829 | Train Loss: 8.7601\n",
            "[Epoch 12] Batch 441/829 | Train Loss: 6.0740\n",
            "[Epoch 12] Batch 451/829 | Train Loss: 7.8243\n",
            "[Epoch 12] Batch 461/829 | Train Loss: 7.4567\n",
            "[Epoch 12] Batch 471/829 | Train Loss: 7.5390\n",
            "[Epoch 12] Batch 481/829 | Train Loss: 7.4392\n",
            "[Epoch 12] Batch 491/829 | Train Loss: 9.6512\n",
            "[Epoch 12] Batch 501/829 | Train Loss: 9.3965\n",
            "[Epoch 12] Batch 511/829 | Train Loss: 6.0675\n",
            "[Epoch 12] Batch 521/829 | Train Loss: 9.3876\n",
            "[Epoch 12] Batch 531/829 | Train Loss: 7.8721\n",
            "[Epoch 12] Batch 541/829 | Train Loss: 6.7052\n",
            "[Epoch 12] Batch 551/829 | Train Loss: 8.1909\n",
            "[Epoch 12] Batch 561/829 | Train Loss: 6.3102\n",
            "[Epoch 12] Batch 571/829 | Train Loss: 7.8487\n",
            "[Epoch 12] Batch 581/829 | Train Loss: 9.3650\n",
            "[Epoch 12] Batch 591/829 | Train Loss: 6.1015\n",
            "[Epoch 12] Batch 601/829 | Train Loss: 9.2327\n",
            "[Epoch 12] Batch 611/829 | Train Loss: 6.1893\n",
            "[Epoch 12] Batch 621/829 | Train Loss: 5.9874\n",
            "[Epoch 12] Batch 631/829 | Train Loss: 7.1860\n",
            "[Epoch 12] Batch 641/829 | Train Loss: 6.7313\n",
            "[Epoch 12] Batch 651/829 | Train Loss: 13.5351\n",
            "[Epoch 12] Batch 661/829 | Train Loss: 8.1838\n",
            "[Epoch 12] Batch 671/829 | Train Loss: 10.1643\n",
            "[Epoch 12] Batch 681/829 | Train Loss: 7.3822\n",
            "[Epoch 12] Batch 691/829 | Train Loss: 5.4522\n",
            "[Epoch 12] Batch 701/829 | Train Loss: 9.1396\n",
            "[Epoch 12] Batch 711/829 | Train Loss: 4.9491\n",
            "[Epoch 12] Batch 721/829 | Train Loss: 9.6783\n",
            "[Epoch 12] Batch 731/829 | Train Loss: 6.7231\n",
            "[Epoch 12] Batch 741/829 | Train Loss: 6.5311\n",
            "[Epoch 12] Batch 751/829 | Train Loss: 5.0669\n",
            "[Epoch 12] Batch 761/829 | Train Loss: 7.7294\n",
            "[Epoch 12] Batch 771/829 | Train Loss: 7.4445\n",
            "[Epoch 12] Batch 781/829 | Train Loss: 10.1169\n",
            "[Epoch 12] Batch 791/829 | Train Loss: 5.2729\n",
            "[Epoch 12] Batch 801/829 | Train Loss: 6.6402\n",
            "[Epoch 12] Batch 811/829 | Train Loss: 10.7370\n",
            "[Epoch 12] Batch 821/829 | Train Loss: 7.2713\n",
            "[Epoch 12] Batch 829/829 | Train Loss: 8.4366\n",
            "ðŸŸ¢ Epoch 12 | Avg Train Loss: 7.6918 | Val Loss: 4.5746\n",
            "âœ… Best model saved: models/best_model_resnet18_mlp_aug_smoothl1.pth (Val Loss = 4.5746)\n",
            "ðŸ’¾ Checkpoint saved at epoch 13: models/checkpoint_epoch_13_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 13] Batch 001/829 | Train Loss: 6.6896\n",
            "[Epoch 13] Batch 011/829 | Train Loss: 9.5195\n",
            "[Epoch 13] Batch 021/829 | Train Loss: 11.2370\n",
            "[Epoch 13] Batch 031/829 | Train Loss: 7.2137\n",
            "[Epoch 13] Batch 041/829 | Train Loss: 6.6902\n",
            "[Epoch 13] Batch 051/829 | Train Loss: 9.0052\n",
            "[Epoch 13] Batch 061/829 | Train Loss: 6.8363\n",
            "[Epoch 13] Batch 071/829 | Train Loss: 6.3622\n",
            "[Epoch 13] Batch 081/829 | Train Loss: 6.1607\n",
            "[Epoch 13] Batch 091/829 | Train Loss: 8.1169\n",
            "[Epoch 13] Batch 101/829 | Train Loss: 8.1675\n",
            "[Epoch 13] Batch 111/829 | Train Loss: 12.4484\n",
            "[Epoch 13] Batch 121/829 | Train Loss: 8.8977\n",
            "[Epoch 13] Batch 131/829 | Train Loss: 7.4437\n",
            "[Epoch 13] Batch 141/829 | Train Loss: 6.5518\n",
            "[Epoch 13] Batch 151/829 | Train Loss: 10.3685\n",
            "[Epoch 13] Batch 161/829 | Train Loss: 8.6626\n",
            "[Epoch 13] Batch 171/829 | Train Loss: 8.8680\n",
            "[Epoch 13] Batch 181/829 | Train Loss: 7.5936\n",
            "[Epoch 13] Batch 191/829 | Train Loss: 7.1061\n",
            "[Epoch 13] Batch 201/829 | Train Loss: 4.3893\n",
            "[Epoch 13] Batch 211/829 | Train Loss: 6.3486\n",
            "[Epoch 13] Batch 221/829 | Train Loss: 7.2924\n",
            "[Epoch 13] Batch 231/829 | Train Loss: 9.3185\n",
            "[Epoch 13] Batch 241/829 | Train Loss: 6.8080\n",
            "[Epoch 13] Batch 251/829 | Train Loss: 6.4003\n",
            "[Epoch 13] Batch 261/829 | Train Loss: 8.0097\n",
            "[Epoch 13] Batch 271/829 | Train Loss: 5.7871\n",
            "[Epoch 13] Batch 281/829 | Train Loss: 7.1792\n",
            "[Epoch 13] Batch 291/829 | Train Loss: 7.4758\n",
            "[Epoch 13] Batch 301/829 | Train Loss: 9.7921\n",
            "[Epoch 13] Batch 311/829 | Train Loss: 7.9475\n",
            "[Epoch 13] Batch 321/829 | Train Loss: 6.9589\n",
            "[Epoch 13] Batch 331/829 | Train Loss: 5.1051\n",
            "[Epoch 13] Batch 341/829 | Train Loss: 4.3606\n",
            "[Epoch 13] Batch 351/829 | Train Loss: 9.2991\n",
            "[Epoch 13] Batch 361/829 | Train Loss: 7.7595\n",
            "[Epoch 13] Batch 371/829 | Train Loss: 7.6823\n",
            "[Epoch 13] Batch 381/829 | Train Loss: 8.0930\n",
            "[Epoch 13] Batch 391/829 | Train Loss: 6.3275\n",
            "[Epoch 13] Batch 401/829 | Train Loss: 10.7975\n",
            "[Epoch 13] Batch 411/829 | Train Loss: 9.5269\n",
            "[Epoch 13] Batch 421/829 | Train Loss: 9.0067\n",
            "[Epoch 13] Batch 431/829 | Train Loss: 8.0124\n",
            "[Epoch 13] Batch 441/829 | Train Loss: 4.9259\n",
            "[Epoch 13] Batch 451/829 | Train Loss: 7.2313\n",
            "[Epoch 13] Batch 461/829 | Train Loss: 5.8661\n",
            "[Epoch 13] Batch 471/829 | Train Loss: 8.3177\n",
            "[Epoch 13] Batch 481/829 | Train Loss: 10.1110\n",
            "[Epoch 13] Batch 491/829 | Train Loss: 6.9877\n",
            "[Epoch 13] Batch 501/829 | Train Loss: 6.6307\n",
            "[Epoch 13] Batch 511/829 | Train Loss: 6.6724\n",
            "[Epoch 13] Batch 521/829 | Train Loss: 6.1603\n",
            "[Epoch 13] Batch 531/829 | Train Loss: 11.5709\n",
            "[Epoch 13] Batch 541/829 | Train Loss: 9.4403\n",
            "[Epoch 13] Batch 551/829 | Train Loss: 7.5827\n",
            "[Epoch 13] Batch 561/829 | Train Loss: 7.9895\n",
            "[Epoch 13] Batch 571/829 | Train Loss: 5.8597\n",
            "[Epoch 13] Batch 581/829 | Train Loss: 6.6407\n",
            "[Epoch 13] Batch 591/829 | Train Loss: 9.1980\n",
            "[Epoch 13] Batch 601/829 | Train Loss: 7.7232\n",
            "[Epoch 13] Batch 611/829 | Train Loss: 7.0685\n",
            "[Epoch 13] Batch 621/829 | Train Loss: 8.2746\n",
            "[Epoch 13] Batch 631/829 | Train Loss: 7.1559\n",
            "[Epoch 13] Batch 641/829 | Train Loss: 9.5527\n",
            "[Epoch 13] Batch 651/829 | Train Loss: 5.4478\n",
            "[Epoch 13] Batch 661/829 | Train Loss: 5.9762\n",
            "[Epoch 13] Batch 671/829 | Train Loss: 7.4264\n",
            "[Epoch 13] Batch 681/829 | Train Loss: 7.0810\n",
            "[Epoch 13] Batch 691/829 | Train Loss: 7.4347\n",
            "[Epoch 13] Batch 701/829 | Train Loss: 9.8910\n",
            "[Epoch 13] Batch 711/829 | Train Loss: 5.7095\n",
            "[Epoch 13] Batch 721/829 | Train Loss: 8.2200\n",
            "[Epoch 13] Batch 731/829 | Train Loss: 8.9010\n",
            "[Epoch 13] Batch 741/829 | Train Loss: 8.1479\n",
            "[Epoch 13] Batch 751/829 | Train Loss: 5.9372\n",
            "[Epoch 13] Batch 761/829 | Train Loss: 5.3382\n",
            "[Epoch 13] Batch 771/829 | Train Loss: 5.1456\n",
            "[Epoch 13] Batch 781/829 | Train Loss: 6.5839\n",
            "[Epoch 13] Batch 791/829 | Train Loss: 6.5986\n",
            "[Epoch 13] Batch 801/829 | Train Loss: 9.4291\n",
            "[Epoch 13] Batch 811/829 | Train Loss: 9.4463\n",
            "[Epoch 13] Batch 821/829 | Train Loss: 8.3290\n",
            "[Epoch 13] Batch 829/829 | Train Loss: 6.0097\n",
            "ðŸŸ¢ Epoch 13 | Avg Train Loss: 7.6393 | Val Loss: 5.0920\n",
            "ðŸ’¾ Checkpoint saved at epoch 14: models/checkpoint_epoch_14_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 14] Batch 001/829 | Train Loss: 6.5966\n",
            "[Epoch 14] Batch 011/829 | Train Loss: 5.9104\n",
            "[Epoch 14] Batch 021/829 | Train Loss: 7.1221\n",
            "[Epoch 14] Batch 031/829 | Train Loss: 7.5371\n",
            "[Epoch 14] Batch 041/829 | Train Loss: 6.5384\n",
            "[Epoch 14] Batch 051/829 | Train Loss: 7.4310\n",
            "[Epoch 14] Batch 061/829 | Train Loss: 6.6219\n",
            "[Epoch 14] Batch 071/829 | Train Loss: 6.7970\n",
            "[Epoch 14] Batch 081/829 | Train Loss: 8.3572\n",
            "[Epoch 14] Batch 091/829 | Train Loss: 11.2067\n",
            "[Epoch 14] Batch 101/829 | Train Loss: 7.7685\n",
            "[Epoch 14] Batch 111/829 | Train Loss: 11.6713\n",
            "[Epoch 14] Batch 121/829 | Train Loss: 6.3950\n",
            "[Epoch 14] Batch 131/829 | Train Loss: 8.4639\n",
            "[Epoch 14] Batch 141/829 | Train Loss: 8.9795\n",
            "[Epoch 14] Batch 151/829 | Train Loss: 10.9584\n",
            "[Epoch 14] Batch 161/829 | Train Loss: 5.7140\n",
            "[Epoch 14] Batch 171/829 | Train Loss: 8.8253\n",
            "[Epoch 14] Batch 181/829 | Train Loss: 8.5993\n",
            "[Epoch 14] Batch 191/829 | Train Loss: 7.6080\n",
            "[Epoch 14] Batch 201/829 | Train Loss: 8.3580\n",
            "[Epoch 14] Batch 211/829 | Train Loss: 7.6935\n",
            "[Epoch 14] Batch 221/829 | Train Loss: 5.7565\n",
            "[Epoch 14] Batch 231/829 | Train Loss: 5.5562\n",
            "[Epoch 14] Batch 241/829 | Train Loss: 6.4062\n",
            "[Epoch 14] Batch 251/829 | Train Loss: 7.4262\n",
            "[Epoch 14] Batch 261/829 | Train Loss: 5.7473\n",
            "[Epoch 14] Batch 271/829 | Train Loss: 7.5845\n",
            "[Epoch 14] Batch 281/829 | Train Loss: 6.4607\n",
            "[Epoch 14] Batch 291/829 | Train Loss: 7.1654\n",
            "[Epoch 14] Batch 301/829 | Train Loss: 8.0382\n",
            "[Epoch 14] Batch 311/829 | Train Loss: 9.7560\n",
            "[Epoch 14] Batch 321/829 | Train Loss: 9.3602\n",
            "[Epoch 14] Batch 331/829 | Train Loss: 7.5044\n",
            "[Epoch 14] Batch 341/829 | Train Loss: 12.2806\n",
            "[Epoch 14] Batch 351/829 | Train Loss: 8.4581\n",
            "[Epoch 14] Batch 361/829 | Train Loss: 5.6214\n",
            "[Epoch 14] Batch 371/829 | Train Loss: 9.2772\n",
            "[Epoch 14] Batch 381/829 | Train Loss: 8.5148\n",
            "[Epoch 14] Batch 391/829 | Train Loss: 7.4584\n",
            "[Epoch 14] Batch 401/829 | Train Loss: 10.3539\n",
            "[Epoch 14] Batch 411/829 | Train Loss: 6.3443\n",
            "[Epoch 14] Batch 421/829 | Train Loss: 7.5041\n",
            "[Epoch 14] Batch 431/829 | Train Loss: 6.7485\n",
            "[Epoch 14] Batch 441/829 | Train Loss: 7.4507\n",
            "[Epoch 14] Batch 451/829 | Train Loss: 9.2228\n",
            "[Epoch 14] Batch 461/829 | Train Loss: 6.2905\n",
            "[Epoch 14] Batch 471/829 | Train Loss: 9.1840\n",
            "[Epoch 14] Batch 481/829 | Train Loss: 6.6711\n",
            "[Epoch 14] Batch 491/829 | Train Loss: 9.7193\n",
            "[Epoch 14] Batch 501/829 | Train Loss: 7.5656\n",
            "[Epoch 14] Batch 511/829 | Train Loss: 7.7892\n",
            "[Epoch 14] Batch 521/829 | Train Loss: 6.9810\n",
            "[Epoch 14] Batch 531/829 | Train Loss: 8.4804\n",
            "[Epoch 14] Batch 541/829 | Train Loss: 6.7426\n",
            "[Epoch 14] Batch 551/829 | Train Loss: 9.8882\n",
            "[Epoch 14] Batch 561/829 | Train Loss: 6.7503\n",
            "[Epoch 14] Batch 571/829 | Train Loss: 6.9746\n",
            "[Epoch 14] Batch 581/829 | Train Loss: 7.8000\n",
            "[Epoch 14] Batch 591/829 | Train Loss: 6.5315\n",
            "[Epoch 14] Batch 601/829 | Train Loss: 5.1851\n",
            "[Epoch 14] Batch 611/829 | Train Loss: 8.3048\n",
            "[Epoch 14] Batch 621/829 | Train Loss: 6.6303\n",
            "[Epoch 14] Batch 631/829 | Train Loss: 8.0339\n",
            "[Epoch 14] Batch 641/829 | Train Loss: 7.5337\n",
            "[Epoch 14] Batch 651/829 | Train Loss: 10.8291\n",
            "[Epoch 14] Batch 661/829 | Train Loss: 5.8005\n",
            "[Epoch 14] Batch 671/829 | Train Loss: 9.8283\n",
            "[Epoch 14] Batch 681/829 | Train Loss: 10.0816\n",
            "[Epoch 14] Batch 691/829 | Train Loss: 8.9227\n",
            "[Epoch 14] Batch 701/829 | Train Loss: 9.2137\n",
            "[Epoch 14] Batch 711/829 | Train Loss: 7.7811\n",
            "[Epoch 14] Batch 721/829 | Train Loss: 6.5320\n",
            "[Epoch 14] Batch 731/829 | Train Loss: 6.2809\n",
            "[Epoch 14] Batch 741/829 | Train Loss: 8.4106\n",
            "[Epoch 14] Batch 751/829 | Train Loss: 7.3278\n",
            "[Epoch 14] Batch 761/829 | Train Loss: 8.2588\n",
            "[Epoch 14] Batch 771/829 | Train Loss: 9.3676\n",
            "[Epoch 14] Batch 781/829 | Train Loss: 6.0525\n",
            "[Epoch 14] Batch 791/829 | Train Loss: 9.4887\n",
            "[Epoch 14] Batch 801/829 | Train Loss: 5.6959\n",
            "[Epoch 14] Batch 811/829 | Train Loss: 6.9381\n",
            "[Epoch 14] Batch 821/829 | Train Loss: 5.6275\n",
            "[Epoch 14] Batch 829/829 | Train Loss: 7.8686\n",
            "ðŸŸ¢ Epoch 14 | Avg Train Loss: 7.6298 | Val Loss: 4.5921\n",
            "ðŸ’¾ Checkpoint saved at epoch 15: models/checkpoint_epoch_15_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 15] Batch 001/829 | Train Loss: 6.8664\n",
            "[Epoch 15] Batch 011/829 | Train Loss: 8.0328\n",
            "[Epoch 15] Batch 021/829 | Train Loss: 5.8073\n",
            "[Epoch 15] Batch 031/829 | Train Loss: 7.0049\n",
            "[Epoch 15] Batch 041/829 | Train Loss: 7.0929\n",
            "[Epoch 15] Batch 051/829 | Train Loss: 8.5793\n",
            "[Epoch 15] Batch 061/829 | Train Loss: 7.6272\n",
            "[Epoch 15] Batch 071/829 | Train Loss: 6.8983\n",
            "[Epoch 15] Batch 081/829 | Train Loss: 8.0814\n",
            "[Epoch 15] Batch 091/829 | Train Loss: 7.4533\n",
            "[Epoch 15] Batch 101/829 | Train Loss: 9.7852\n",
            "[Epoch 15] Batch 111/829 | Train Loss: 8.5609\n",
            "[Epoch 15] Batch 121/829 | Train Loss: 8.4472\n",
            "[Epoch 15] Batch 131/829 | Train Loss: 6.8004\n",
            "[Epoch 15] Batch 141/829 | Train Loss: 7.0478\n",
            "[Epoch 15] Batch 151/829 | Train Loss: 4.5160\n",
            "[Epoch 15] Batch 161/829 | Train Loss: 8.9781\n",
            "[Epoch 15] Batch 171/829 | Train Loss: 8.0905\n",
            "[Epoch 15] Batch 181/829 | Train Loss: 7.6183\n",
            "[Epoch 15] Batch 191/829 | Train Loss: 7.1969\n",
            "[Epoch 15] Batch 201/829 | Train Loss: 6.3385\n",
            "[Epoch 15] Batch 211/829 | Train Loss: 9.3051\n",
            "[Epoch 15] Batch 221/829 | Train Loss: 6.8352\n",
            "[Epoch 15] Batch 231/829 | Train Loss: 8.2222\n",
            "[Epoch 15] Batch 241/829 | Train Loss: 7.5169\n",
            "[Epoch 15] Batch 251/829 | Train Loss: 8.0901\n",
            "[Epoch 15] Batch 261/829 | Train Loss: 7.9704\n",
            "[Epoch 15] Batch 271/829 | Train Loss: 9.3686\n",
            "[Epoch 15] Batch 281/829 | Train Loss: 7.2907\n",
            "[Epoch 15] Batch 291/829 | Train Loss: 8.8271\n",
            "[Epoch 15] Batch 301/829 | Train Loss: 5.1691\n",
            "[Epoch 15] Batch 311/829 | Train Loss: 10.4108\n",
            "[Epoch 15] Batch 321/829 | Train Loss: 9.1238\n",
            "[Epoch 15] Batch 331/829 | Train Loss: 5.9933\n",
            "[Epoch 15] Batch 341/829 | Train Loss: 6.3667\n",
            "[Epoch 15] Batch 351/829 | Train Loss: 6.2398\n",
            "[Epoch 15] Batch 361/829 | Train Loss: 8.4978\n",
            "[Epoch 15] Batch 371/829 | Train Loss: 7.3576\n",
            "[Epoch 15] Batch 381/829 | Train Loss: 7.8245\n",
            "[Epoch 15] Batch 391/829 | Train Loss: 9.0881\n",
            "[Epoch 15] Batch 401/829 | Train Loss: 8.9438\n",
            "[Epoch 15] Batch 411/829 | Train Loss: 7.5776\n",
            "[Epoch 15] Batch 421/829 | Train Loss: 9.0235\n",
            "[Epoch 15] Batch 431/829 | Train Loss: 8.7559\n",
            "[Epoch 15] Batch 441/829 | Train Loss: 7.1035\n",
            "[Epoch 15] Batch 451/829 | Train Loss: 7.8186\n",
            "[Epoch 15] Batch 461/829 | Train Loss: 6.4214\n",
            "[Epoch 15] Batch 471/829 | Train Loss: 8.0831\n",
            "[Epoch 15] Batch 481/829 | Train Loss: 8.3984\n",
            "[Epoch 15] Batch 491/829 | Train Loss: 10.3320\n",
            "[Epoch 15] Batch 501/829 | Train Loss: 7.8551\n",
            "[Epoch 15] Batch 511/829 | Train Loss: 5.0613\n",
            "[Epoch 15] Batch 521/829 | Train Loss: 6.4460\n",
            "[Epoch 15] Batch 531/829 | Train Loss: 6.4184\n",
            "[Epoch 15] Batch 541/829 | Train Loss: 6.2289\n",
            "[Epoch 15] Batch 551/829 | Train Loss: 7.6825\n",
            "[Epoch 15] Batch 561/829 | Train Loss: 7.2319\n",
            "[Epoch 15] Batch 571/829 | Train Loss: 5.7085\n",
            "[Epoch 15] Batch 581/829 | Train Loss: 6.4650\n",
            "[Epoch 15] Batch 591/829 | Train Loss: 8.6651\n",
            "[Epoch 15] Batch 601/829 | Train Loss: 7.4095\n",
            "[Epoch 15] Batch 611/829 | Train Loss: 9.6797\n",
            "[Epoch 15] Batch 621/829 | Train Loss: 7.7534\n",
            "[Epoch 15] Batch 631/829 | Train Loss: 7.3775\n",
            "[Epoch 15] Batch 641/829 | Train Loss: 9.8850\n",
            "[Epoch 15] Batch 651/829 | Train Loss: 7.1323\n",
            "[Epoch 15] Batch 661/829 | Train Loss: 7.7203\n",
            "[Epoch 15] Batch 671/829 | Train Loss: 6.9175\n",
            "[Epoch 15] Batch 681/829 | Train Loss: 8.9780\n",
            "[Epoch 15] Batch 691/829 | Train Loss: 6.0377\n",
            "[Epoch 15] Batch 701/829 | Train Loss: 11.6712\n",
            "[Epoch 15] Batch 711/829 | Train Loss: 7.0197\n",
            "[Epoch 15] Batch 721/829 | Train Loss: 8.8918\n",
            "[Epoch 15] Batch 731/829 | Train Loss: 8.4056\n",
            "[Epoch 15] Batch 741/829 | Train Loss: 9.2173\n",
            "[Epoch 15] Batch 751/829 | Train Loss: 7.2788\n",
            "[Epoch 15] Batch 761/829 | Train Loss: 7.8553\n",
            "[Epoch 15] Batch 771/829 | Train Loss: 8.3750\n",
            "[Epoch 15] Batch 781/829 | Train Loss: 8.2342\n",
            "[Epoch 15] Batch 791/829 | Train Loss: 5.9888\n",
            "[Epoch 15] Batch 801/829 | Train Loss: 8.1631\n",
            "[Epoch 15] Batch 811/829 | Train Loss: 7.1475\n",
            "[Epoch 15] Batch 821/829 | Train Loss: 9.1313\n",
            "[Epoch 15] Batch 829/829 | Train Loss: 8.7024\n",
            "ðŸŸ¢ Epoch 15 | Avg Train Loss: 7.6472 | Val Loss: 5.4474\n",
            "ðŸ’¾ Checkpoint saved at epoch 16: models/checkpoint_epoch_16_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 16] Batch 001/829 | Train Loss: 5.4163\n",
            "[Epoch 16] Batch 011/829 | Train Loss: 8.5246\n",
            "[Epoch 16] Batch 021/829 | Train Loss: 9.6914\n",
            "[Epoch 16] Batch 031/829 | Train Loss: 6.8976\n",
            "[Epoch 16] Batch 041/829 | Train Loss: 9.3837\n",
            "[Epoch 16] Batch 051/829 | Train Loss: 6.1189\n",
            "[Epoch 16] Batch 061/829 | Train Loss: 5.0370\n",
            "[Epoch 16] Batch 071/829 | Train Loss: 4.5415\n",
            "[Epoch 16] Batch 081/829 | Train Loss: 8.4689\n",
            "[Epoch 16] Batch 091/829 | Train Loss: 6.6592\n",
            "[Epoch 16] Batch 101/829 | Train Loss: 7.4751\n",
            "[Epoch 16] Batch 111/829 | Train Loss: 6.9771\n",
            "[Epoch 16] Batch 121/829 | Train Loss: 7.6418\n",
            "[Epoch 16] Batch 131/829 | Train Loss: 8.4279\n",
            "[Epoch 16] Batch 141/829 | Train Loss: 7.2946\n",
            "[Epoch 16] Batch 151/829 | Train Loss: 5.6078\n",
            "[Epoch 16] Batch 161/829 | Train Loss: 8.2346\n",
            "[Epoch 16] Batch 171/829 | Train Loss: 6.9699\n",
            "[Epoch 16] Batch 181/829 | Train Loss: 7.4303\n",
            "[Epoch 16] Batch 191/829 | Train Loss: 10.0255\n",
            "[Epoch 16] Batch 201/829 | Train Loss: 8.5565\n",
            "[Epoch 16] Batch 211/829 | Train Loss: 8.7804\n",
            "[Epoch 16] Batch 221/829 | Train Loss: 8.6063\n",
            "[Epoch 16] Batch 231/829 | Train Loss: 7.2244\n",
            "[Epoch 16] Batch 241/829 | Train Loss: 6.0778\n",
            "[Epoch 16] Batch 251/829 | Train Loss: 6.5873\n",
            "[Epoch 16] Batch 261/829 | Train Loss: 8.0667\n",
            "[Epoch 16] Batch 271/829 | Train Loss: 9.7985\n",
            "[Epoch 16] Batch 281/829 | Train Loss: 8.6375\n",
            "[Epoch 16] Batch 291/829 | Train Loss: 7.3064\n",
            "[Epoch 16] Batch 301/829 | Train Loss: 8.8719\n",
            "[Epoch 16] Batch 311/829 | Train Loss: 8.8089\n",
            "[Epoch 16] Batch 321/829 | Train Loss: 7.6785\n",
            "[Epoch 16] Batch 331/829 | Train Loss: 6.1189\n",
            "[Epoch 16] Batch 341/829 | Train Loss: 7.0773\n",
            "[Epoch 16] Batch 351/829 | Train Loss: 8.5003\n",
            "[Epoch 16] Batch 361/829 | Train Loss: 7.6379\n",
            "[Epoch 16] Batch 371/829 | Train Loss: 7.8765\n",
            "[Epoch 16] Batch 381/829 | Train Loss: 6.0844\n",
            "[Epoch 16] Batch 391/829 | Train Loss: 8.5153\n",
            "[Epoch 16] Batch 401/829 | Train Loss: 8.3112\n",
            "[Epoch 16] Batch 411/829 | Train Loss: 7.9939\n",
            "[Epoch 16] Batch 421/829 | Train Loss: 8.1905\n",
            "[Epoch 16] Batch 431/829 | Train Loss: 5.8266\n",
            "[Epoch 16] Batch 441/829 | Train Loss: 7.1844\n",
            "[Epoch 16] Batch 451/829 | Train Loss: 7.9046\n",
            "[Epoch 16] Batch 461/829 | Train Loss: 5.7942\n",
            "[Epoch 16] Batch 471/829 | Train Loss: 5.8620\n",
            "[Epoch 16] Batch 481/829 | Train Loss: 8.8101\n",
            "[Epoch 16] Batch 491/829 | Train Loss: 8.8595\n",
            "[Epoch 16] Batch 501/829 | Train Loss: 6.9076\n",
            "[Epoch 16] Batch 511/829 | Train Loss: 7.0498\n",
            "[Epoch 16] Batch 521/829 | Train Loss: 8.3155\n",
            "[Epoch 16] Batch 531/829 | Train Loss: 7.6261\n",
            "[Epoch 16] Batch 541/829 | Train Loss: 7.5218\n",
            "[Epoch 16] Batch 551/829 | Train Loss: 5.3411\n",
            "[Epoch 16] Batch 561/829 | Train Loss: 11.3716\n",
            "[Epoch 16] Batch 571/829 | Train Loss: 6.1543\n",
            "[Epoch 16] Batch 581/829 | Train Loss: 4.6984\n",
            "[Epoch 16] Batch 591/829 | Train Loss: 7.3613\n",
            "[Epoch 16] Batch 601/829 | Train Loss: 7.7845\n",
            "[Epoch 16] Batch 611/829 | Train Loss: 4.9894\n",
            "[Epoch 16] Batch 621/829 | Train Loss: 8.0601\n",
            "[Epoch 16] Batch 631/829 | Train Loss: 9.2217\n",
            "[Epoch 16] Batch 641/829 | Train Loss: 7.2402\n",
            "[Epoch 16] Batch 651/829 | Train Loss: 6.1520\n",
            "[Epoch 16] Batch 661/829 | Train Loss: 8.5885\n",
            "[Epoch 16] Batch 671/829 | Train Loss: 6.8327\n",
            "[Epoch 16] Batch 681/829 | Train Loss: 8.8911\n",
            "[Epoch 16] Batch 691/829 | Train Loss: 7.9222\n",
            "[Epoch 16] Batch 701/829 | Train Loss: 8.8029\n",
            "[Epoch 16] Batch 711/829 | Train Loss: 11.0083\n",
            "[Epoch 16] Batch 721/829 | Train Loss: 8.8120\n",
            "[Epoch 16] Batch 731/829 | Train Loss: 8.1795\n",
            "[Epoch 16] Batch 741/829 | Train Loss: 7.0117\n",
            "[Epoch 16] Batch 751/829 | Train Loss: 8.1275\n",
            "[Epoch 16] Batch 761/829 | Train Loss: 6.4007\n",
            "[Epoch 16] Batch 771/829 | Train Loss: 7.7113\n",
            "[Epoch 16] Batch 781/829 | Train Loss: 6.8898\n",
            "[Epoch 16] Batch 791/829 | Train Loss: 7.9253\n",
            "[Epoch 16] Batch 801/829 | Train Loss: 7.2995\n",
            "[Epoch 16] Batch 811/829 | Train Loss: 8.3089\n",
            "[Epoch 16] Batch 821/829 | Train Loss: 6.3679\n",
            "[Epoch 16] Batch 829/829 | Train Loss: 12.0902\n",
            "ðŸŸ¢ Epoch 16 | Avg Train Loss: 7.6735 | Val Loss: 5.2285\n",
            "ðŸ’¾ Checkpoint saved at epoch 17: models/checkpoint_epoch_17_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 17] Batch 001/829 | Train Loss: 7.7454\n",
            "[Epoch 17] Batch 011/829 | Train Loss: 7.7445\n",
            "[Epoch 17] Batch 021/829 | Train Loss: 7.6383\n",
            "[Epoch 17] Batch 031/829 | Train Loss: 7.0796\n",
            "[Epoch 17] Batch 041/829 | Train Loss: 7.6283\n",
            "[Epoch 17] Batch 051/829 | Train Loss: 7.0526\n",
            "[Epoch 17] Batch 061/829 | Train Loss: 7.1412\n",
            "[Epoch 17] Batch 071/829 | Train Loss: 8.2547\n",
            "[Epoch 17] Batch 081/829 | Train Loss: 7.4801\n",
            "[Epoch 17] Batch 091/829 | Train Loss: 6.2790\n",
            "[Epoch 17] Batch 101/829 | Train Loss: 8.3998\n",
            "[Epoch 17] Batch 111/829 | Train Loss: 9.0257\n",
            "[Epoch 17] Batch 121/829 | Train Loss: 9.4967\n",
            "[Epoch 17] Batch 131/829 | Train Loss: 8.4106\n",
            "[Epoch 17] Batch 141/829 | Train Loss: 8.0088\n",
            "[Epoch 17] Batch 151/829 | Train Loss: 7.5619\n",
            "[Epoch 17] Batch 161/829 | Train Loss: 7.3496\n",
            "[Epoch 17] Batch 171/829 | Train Loss: 8.6050\n",
            "[Epoch 17] Batch 181/829 | Train Loss: 7.3138\n",
            "[Epoch 17] Batch 191/829 | Train Loss: 5.7667\n",
            "[Epoch 17] Batch 201/829 | Train Loss: 9.0513\n",
            "[Epoch 17] Batch 211/829 | Train Loss: 6.6148\n",
            "[Epoch 17] Batch 221/829 | Train Loss: 7.4177\n",
            "[Epoch 17] Batch 231/829 | Train Loss: 7.2453\n",
            "[Epoch 17] Batch 241/829 | Train Loss: 10.6213\n",
            "[Epoch 17] Batch 251/829 | Train Loss: 7.0993\n",
            "[Epoch 17] Batch 261/829 | Train Loss: 6.3702\n",
            "[Epoch 17] Batch 271/829 | Train Loss: 8.7786\n",
            "[Epoch 17] Batch 281/829 | Train Loss: 7.0489\n",
            "[Epoch 17] Batch 291/829 | Train Loss: 6.1733\n",
            "[Epoch 17] Batch 301/829 | Train Loss: 6.0668\n",
            "[Epoch 17] Batch 311/829 | Train Loss: 6.8971\n",
            "[Epoch 17] Batch 321/829 | Train Loss: 7.7599\n",
            "[Epoch 17] Batch 331/829 | Train Loss: 5.6658\n",
            "[Epoch 17] Batch 341/829 | Train Loss: 8.4232\n",
            "[Epoch 17] Batch 351/829 | Train Loss: 10.6942\n",
            "[Epoch 17] Batch 361/829 | Train Loss: 9.0300\n",
            "[Epoch 17] Batch 371/829 | Train Loss: 8.3494\n",
            "[Epoch 17] Batch 381/829 | Train Loss: 4.9243\n",
            "[Epoch 17] Batch 391/829 | Train Loss: 10.8055\n",
            "[Epoch 17] Batch 401/829 | Train Loss: 6.6731\n",
            "[Epoch 17] Batch 411/829 | Train Loss: 8.0044\n",
            "[Epoch 17] Batch 421/829 | Train Loss: 8.1544\n",
            "[Epoch 17] Batch 431/829 | Train Loss: 9.1570\n",
            "[Epoch 17] Batch 441/829 | Train Loss: 8.2663\n",
            "[Epoch 17] Batch 451/829 | Train Loss: 6.1545\n",
            "[Epoch 17] Batch 461/829 | Train Loss: 5.7222\n",
            "[Epoch 17] Batch 471/829 | Train Loss: 6.4058\n",
            "[Epoch 17] Batch 481/829 | Train Loss: 7.1992\n",
            "[Epoch 17] Batch 491/829 | Train Loss: 8.0236\n",
            "[Epoch 17] Batch 501/829 | Train Loss: 8.9694\n",
            "[Epoch 17] Batch 511/829 | Train Loss: 8.6247\n",
            "[Epoch 17] Batch 521/829 | Train Loss: 8.9299\n",
            "[Epoch 17] Batch 531/829 | Train Loss: 8.6396\n",
            "[Epoch 17] Batch 541/829 | Train Loss: 8.5443\n",
            "[Epoch 17] Batch 551/829 | Train Loss: 6.0297\n",
            "[Epoch 17] Batch 561/829 | Train Loss: 8.4965\n",
            "[Epoch 17] Batch 571/829 | Train Loss: 5.8435\n",
            "[Epoch 17] Batch 581/829 | Train Loss: 8.5429\n",
            "[Epoch 17] Batch 591/829 | Train Loss: 8.9058\n",
            "[Epoch 17] Batch 601/829 | Train Loss: 8.3461\n",
            "[Epoch 17] Batch 611/829 | Train Loss: 7.4803\n",
            "[Epoch 17] Batch 621/829 | Train Loss: 6.5721\n",
            "[Epoch 17] Batch 631/829 | Train Loss: 7.2776\n",
            "[Epoch 17] Batch 641/829 | Train Loss: 6.3353\n",
            "[Epoch 17] Batch 651/829 | Train Loss: 6.5079\n",
            "[Epoch 17] Batch 661/829 | Train Loss: 6.2223\n",
            "[Epoch 17] Batch 671/829 | Train Loss: 5.2566\n",
            "[Epoch 17] Batch 681/829 | Train Loss: 8.0014\n",
            "[Epoch 17] Batch 691/829 | Train Loss: 9.4896\n",
            "[Epoch 17] Batch 701/829 | Train Loss: 6.0005\n",
            "[Epoch 17] Batch 711/829 | Train Loss: 7.3364\n",
            "[Epoch 17] Batch 721/829 | Train Loss: 5.7693\n",
            "[Epoch 17] Batch 731/829 | Train Loss: 11.2541\n",
            "[Epoch 17] Batch 741/829 | Train Loss: 5.6549\n",
            "[Epoch 17] Batch 751/829 | Train Loss: 7.3740\n",
            "[Epoch 17] Batch 761/829 | Train Loss: 10.4660\n",
            "[Epoch 17] Batch 771/829 | Train Loss: 8.4119\n",
            "[Epoch 17] Batch 781/829 | Train Loss: 8.5633\n",
            "[Epoch 17] Batch 791/829 | Train Loss: 9.3920\n",
            "[Epoch 17] Batch 801/829 | Train Loss: 7.6433\n",
            "[Epoch 17] Batch 811/829 | Train Loss: 5.9014\n",
            "[Epoch 17] Batch 821/829 | Train Loss: 7.5510\n",
            "[Epoch 17] Batch 829/829 | Train Loss: 8.4841\n",
            "ðŸŸ¢ Epoch 17 | Avg Train Loss: 7.7030 | Val Loss: 4.6026\n",
            "ðŸ’¾ Checkpoint saved at epoch 18: models/checkpoint_epoch_18_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 18] Batch 001/829 | Train Loss: 9.0621\n",
            "[Epoch 18] Batch 011/829 | Train Loss: 10.4283\n",
            "[Epoch 18] Batch 021/829 | Train Loss: 5.0503\n",
            "[Epoch 18] Batch 031/829 | Train Loss: 6.4888\n",
            "[Epoch 18] Batch 041/829 | Train Loss: 7.2233\n",
            "[Epoch 18] Batch 051/829 | Train Loss: 8.4906\n",
            "[Epoch 18] Batch 061/829 | Train Loss: 6.0487\n",
            "[Epoch 18] Batch 071/829 | Train Loss: 8.0377\n",
            "[Epoch 18] Batch 081/829 | Train Loss: 9.1936\n",
            "[Epoch 18] Batch 091/829 | Train Loss: 7.9023\n",
            "[Epoch 18] Batch 101/829 | Train Loss: 5.9657\n",
            "[Epoch 18] Batch 111/829 | Train Loss: 5.5999\n",
            "[Epoch 18] Batch 121/829 | Train Loss: 8.0179\n",
            "[Epoch 18] Batch 131/829 | Train Loss: 8.3001\n",
            "[Epoch 18] Batch 141/829 | Train Loss: 7.0337\n",
            "[Epoch 18] Batch 151/829 | Train Loss: 5.7803\n",
            "[Epoch 18] Batch 161/829 | Train Loss: 5.8053\n",
            "[Epoch 18] Batch 171/829 | Train Loss: 9.6621\n",
            "[Epoch 18] Batch 181/829 | Train Loss: 6.4505\n",
            "[Epoch 18] Batch 191/829 | Train Loss: 9.4561\n",
            "[Epoch 18] Batch 201/829 | Train Loss: 10.3091\n",
            "[Epoch 18] Batch 211/829 | Train Loss: 6.3890\n",
            "[Epoch 18] Batch 221/829 | Train Loss: 7.0993\n",
            "[Epoch 18] Batch 231/829 | Train Loss: 6.1664\n",
            "[Epoch 18] Batch 241/829 | Train Loss: 9.6801\n",
            "[Epoch 18] Batch 251/829 | Train Loss: 5.7478\n",
            "[Epoch 18] Batch 261/829 | Train Loss: 8.9501\n",
            "[Epoch 18] Batch 271/829 | Train Loss: 6.2990\n",
            "[Epoch 18] Batch 281/829 | Train Loss: 6.5371\n",
            "[Epoch 18] Batch 291/829 | Train Loss: 6.5423\n",
            "[Epoch 18] Batch 301/829 | Train Loss: 7.2068\n",
            "[Epoch 18] Batch 311/829 | Train Loss: 7.4675\n",
            "[Epoch 18] Batch 321/829 | Train Loss: 5.7337\n",
            "[Epoch 18] Batch 331/829 | Train Loss: 7.1510\n",
            "[Epoch 18] Batch 341/829 | Train Loss: 8.6245\n",
            "[Epoch 18] Batch 351/829 | Train Loss: 6.2956\n",
            "[Epoch 18] Batch 361/829 | Train Loss: 7.3116\n",
            "[Epoch 18] Batch 371/829 | Train Loss: 6.0603\n",
            "[Epoch 18] Batch 381/829 | Train Loss: 10.2458\n",
            "[Epoch 18] Batch 391/829 | Train Loss: 7.1478\n",
            "[Epoch 18] Batch 401/829 | Train Loss: 7.0886\n",
            "[Epoch 18] Batch 411/829 | Train Loss: 9.7599\n",
            "[Epoch 18] Batch 421/829 | Train Loss: 6.6651\n",
            "[Epoch 18] Batch 431/829 | Train Loss: 6.3573\n",
            "[Epoch 18] Batch 441/829 | Train Loss: 10.7392\n",
            "[Epoch 18] Batch 451/829 | Train Loss: 8.2951\n",
            "[Epoch 18] Batch 461/829 | Train Loss: 9.6492\n",
            "[Epoch 18] Batch 471/829 | Train Loss: 6.9397\n",
            "[Epoch 18] Batch 481/829 | Train Loss: 6.0960\n",
            "[Epoch 18] Batch 491/829 | Train Loss: 6.5525\n",
            "[Epoch 18] Batch 501/829 | Train Loss: 6.4902\n",
            "[Epoch 18] Batch 511/829 | Train Loss: 7.0544\n",
            "[Epoch 18] Batch 521/829 | Train Loss: 9.1784\n",
            "[Epoch 18] Batch 531/829 | Train Loss: 6.2582\n",
            "[Epoch 18] Batch 541/829 | Train Loss: 7.4858\n",
            "[Epoch 18] Batch 551/829 | Train Loss: 7.4818\n",
            "[Epoch 18] Batch 561/829 | Train Loss: 10.0567\n",
            "[Epoch 18] Batch 571/829 | Train Loss: 8.8316\n",
            "[Epoch 18] Batch 581/829 | Train Loss: 5.9307\n",
            "[Epoch 18] Batch 591/829 | Train Loss: 12.1824\n",
            "[Epoch 18] Batch 601/829 | Train Loss: 6.4501\n",
            "[Epoch 18] Batch 611/829 | Train Loss: 5.0018\n",
            "[Epoch 18] Batch 621/829 | Train Loss: 9.0908\n",
            "[Epoch 18] Batch 631/829 | Train Loss: 8.8360\n",
            "[Epoch 18] Batch 641/829 | Train Loss: 6.4694\n",
            "[Epoch 18] Batch 651/829 | Train Loss: 4.7634\n",
            "[Epoch 18] Batch 661/829 | Train Loss: 6.7303\n",
            "[Epoch 18] Batch 671/829 | Train Loss: 7.7097\n",
            "[Epoch 18] Batch 681/829 | Train Loss: 5.1242\n",
            "[Epoch 18] Batch 691/829 | Train Loss: 6.6979\n",
            "[Epoch 18] Batch 701/829 | Train Loss: 8.0802\n",
            "[Epoch 18] Batch 711/829 | Train Loss: 7.6092\n",
            "[Epoch 18] Batch 721/829 | Train Loss: 6.7300\n",
            "[Epoch 18] Batch 731/829 | Train Loss: 8.1795\n",
            "[Epoch 18] Batch 741/829 | Train Loss: 6.4771\n",
            "[Epoch 18] Batch 751/829 | Train Loss: 6.8598\n",
            "[Epoch 18] Batch 761/829 | Train Loss: 7.0572\n",
            "[Epoch 18] Batch 771/829 | Train Loss: 8.9351\n",
            "[Epoch 18] Batch 781/829 | Train Loss: 10.4191\n",
            "[Epoch 18] Batch 791/829 | Train Loss: 9.2048\n",
            "[Epoch 18] Batch 801/829 | Train Loss: 8.0151\n",
            "[Epoch 18] Batch 811/829 | Train Loss: 7.5573\n",
            "[Epoch 18] Batch 821/829 | Train Loss: 8.5956\n",
            "[Epoch 18] Batch 829/829 | Train Loss: 6.1718\n",
            "ðŸŸ¢ Epoch 18 | Avg Train Loss: 7.6344 | Val Loss: 4.6234\n",
            "ðŸ’¾ Checkpoint saved at epoch 19: models/checkpoint_epoch_19_resnet18_mlp_aug_smoothl1.pth\n",
            "[Epoch 19] Batch 001/829 | Train Loss: 7.9573\n",
            "[Epoch 19] Batch 011/829 | Train Loss: 8.1656\n",
            "[Epoch 19] Batch 021/829 | Train Loss: 6.6490\n",
            "[Epoch 19] Batch 031/829 | Train Loss: 6.2545\n",
            "[Epoch 19] Batch 041/829 | Train Loss: 6.5972\n",
            "[Epoch 19] Batch 051/829 | Train Loss: 8.8926\n",
            "[Epoch 19] Batch 061/829 | Train Loss: 8.5800\n",
            "[Epoch 19] Batch 071/829 | Train Loss: 7.1914\n",
            "[Epoch 19] Batch 081/829 | Train Loss: 6.6873\n",
            "[Epoch 19] Batch 091/829 | Train Loss: 5.2309\n",
            "[Epoch 19] Batch 101/829 | Train Loss: 7.9642\n",
            "[Epoch 19] Batch 111/829 | Train Loss: 6.8707\n",
            "[Epoch 19] Batch 121/829 | Train Loss: 7.3926\n",
            "[Epoch 19] Batch 131/829 | Train Loss: 7.2730\n",
            "[Epoch 19] Batch 141/829 | Train Loss: 7.9033\n",
            "[Epoch 19] Batch 151/829 | Train Loss: 6.2419\n",
            "[Epoch 19] Batch 161/829 | Train Loss: 6.6963\n",
            "[Epoch 19] Batch 171/829 | Train Loss: 5.8430\n",
            "[Epoch 19] Batch 181/829 | Train Loss: 9.0957\n",
            "[Epoch 19] Batch 191/829 | Train Loss: 8.1663\n",
            "[Epoch 19] Batch 201/829 | Train Loss: 6.4974\n",
            "[Epoch 19] Batch 211/829 | Train Loss: 8.8873\n",
            "[Epoch 19] Batch 221/829 | Train Loss: 7.8334\n",
            "[Epoch 19] Batch 231/829 | Train Loss: 8.2194\n",
            "[Epoch 19] Batch 241/829 | Train Loss: 8.1674\n",
            "[Epoch 19] Batch 251/829 | Train Loss: 7.6043\n",
            "[Epoch 19] Batch 261/829 | Train Loss: 7.5824\n",
            "[Epoch 19] Batch 271/829 | Train Loss: 6.5123\n",
            "[Epoch 19] Batch 281/829 | Train Loss: 7.5607\n",
            "[Epoch 19] Batch 291/829 | Train Loss: 5.9521\n",
            "[Epoch 19] Batch 301/829 | Train Loss: 7.3962\n",
            "[Epoch 19] Batch 311/829 | Train Loss: 10.4909\n",
            "[Epoch 19] Batch 321/829 | Train Loss: 10.1338\n",
            "[Epoch 19] Batch 331/829 | Train Loss: 7.5826\n",
            "[Epoch 19] Batch 341/829 | Train Loss: 8.4991\n",
            "[Epoch 19] Batch 351/829 | Train Loss: 6.4301\n",
            "[Epoch 19] Batch 361/829 | Train Loss: 7.4065\n",
            "[Epoch 19] Batch 371/829 | Train Loss: 6.5135\n",
            "[Epoch 19] Batch 381/829 | Train Loss: 5.9699\n",
            "[Epoch 19] Batch 391/829 | Train Loss: 7.7222\n",
            "[Epoch 19] Batch 401/829 | Train Loss: 7.3998\n",
            "[Epoch 19] Batch 411/829 | Train Loss: 7.6447\n",
            "[Epoch 19] Batch 421/829 | Train Loss: 6.8145\n",
            "[Epoch 19] Batch 431/829 | Train Loss: 7.6318\n",
            "[Epoch 19] Batch 441/829 | Train Loss: 7.1721\n",
            "[Epoch 19] Batch 451/829 | Train Loss: 9.6411\n",
            "[Epoch 19] Batch 461/829 | Train Loss: 9.0942\n",
            "[Epoch 19] Batch 471/829 | Train Loss: 7.4696\n",
            "[Epoch 19] Batch 481/829 | Train Loss: 6.5520\n",
            "[Epoch 19] Batch 491/829 | Train Loss: 7.1244\n",
            "[Epoch 19] Batch 501/829 | Train Loss: 7.0881\n",
            "[Epoch 19] Batch 511/829 | Train Loss: 6.9340\n",
            "[Epoch 19] Batch 521/829 | Train Loss: 5.6519\n",
            "[Epoch 19] Batch 531/829 | Train Loss: 8.2718\n",
            "[Epoch 19] Batch 541/829 | Train Loss: 11.9880\n",
            "[Epoch 19] Batch 551/829 | Train Loss: 12.6184\n",
            "[Epoch 19] Batch 561/829 | Train Loss: 10.2690\n",
            "[Epoch 19] Batch 571/829 | Train Loss: 7.6821\n",
            "[Epoch 19] Batch 581/829 | Train Loss: 5.6363\n",
            "[Epoch 19] Batch 591/829 | Train Loss: 7.7689\n",
            "[Epoch 19] Batch 601/829 | Train Loss: 7.1905\n",
            "[Epoch 19] Batch 611/829 | Train Loss: 8.7416\n",
            "[Epoch 19] Batch 621/829 | Train Loss: 9.5458\n",
            "[Epoch 19] Batch 631/829 | Train Loss: 5.7940\n",
            "[Epoch 19] Batch 641/829 | Train Loss: 7.3236\n",
            "[Epoch 19] Batch 651/829 | Train Loss: 6.0229\n",
            "[Epoch 19] Batch 661/829 | Train Loss: 8.8439\n",
            "[Epoch 19] Batch 671/829 | Train Loss: 8.8554\n",
            "[Epoch 19] Batch 681/829 | Train Loss: 6.6651\n",
            "[Epoch 19] Batch 691/829 | Train Loss: 9.2094\n",
            "[Epoch 19] Batch 701/829 | Train Loss: 5.5362\n",
            "[Epoch 19] Batch 711/829 | Train Loss: 9.2316\n",
            "[Epoch 19] Batch 721/829 | Train Loss: 6.4435\n",
            "[Epoch 19] Batch 731/829 | Train Loss: 6.3498\n",
            "[Epoch 19] Batch 741/829 | Train Loss: 7.2628\n",
            "[Epoch 19] Batch 751/829 | Train Loss: 7.3528\n",
            "[Epoch 19] Batch 761/829 | Train Loss: 7.4755\n",
            "[Epoch 19] Batch 771/829 | Train Loss: 9.4534\n",
            "[Epoch 19] Batch 781/829 | Train Loss: 6.2919\n",
            "[Epoch 19] Batch 791/829 | Train Loss: 8.1614\n",
            "[Epoch 19] Batch 801/829 | Train Loss: 8.3012\n",
            "[Epoch 19] Batch 811/829 | Train Loss: 7.3884\n",
            "[Epoch 19] Batch 821/829 | Train Loss: 7.3541\n",
            "[Epoch 19] Batch 829/829 | Train Loss: 4.1793\n",
            "ðŸŸ¢ Epoch 19 | Avg Train Loss: 7.5264 | Val Loss: 4.5778\n",
            "ðŸ’¾ Checkpoint saved at epoch 20: models/checkpoint_epoch_20_resnet18_mlp_aug_smoothl1.pth\n",
            "ðŸ’¾ Last model saved: models/last_model_resnet18_mlp_aug_smoothl1.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import os\n",
        "\n",
        "# Configura suffisso personalizzato\n",
        "suffix = \"resnet18_mlp_aug_smoothl1\"\n",
        "save_dir = \"models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "best_model_path = os.path.join(save_dir, f\"best_model_{suffix}.pth\")\n",
        "last_model_path = os.path.join(save_dir, f\"last_model_{suffix}.pth\")\n",
        "\n",
        "# Costruzione modello\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 28)\n",
        ")\n",
        "\n",
        "# Stampa parametri\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss, optimizer\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 20\n",
        "\n",
        "# Funzione di validazione\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for imgs, kps in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        kps = kps.to(device)\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, kps)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# Training\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_batches = len(train_loader)\n",
        "\n",
        "    for i, (imgs, kps) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        kps = kps.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, kps)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0 or i == total_batches - 1:\n",
        "            print(f\"[Epoch {epoch:02d}] Batch {i+1:03d}/{total_batches} | Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / total_batches\n",
        "    val_loss = validate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"ðŸŸ¢ Epoch {epoch:02d} | Avg Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Salva best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"âœ… Best model saved: {best_model_path} (Val Loss = {val_loss:.4f})\")\n",
        "\n",
        "    # Salvataggio periodico ogni 1 epoche\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1:02d}_{suffix}.pth\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"ðŸ’¾ Checkpoint saved at epoch {epoch+1}: {checkpoint_path}\")\n",
        "\n",
        "# Salva modello finale\n",
        "torch.save(model.state_dict(), last_model_path)\n",
        "print(f\"ðŸ’¾ Last model saved: {last_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL4fDpXh3j-V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SXXMhfyCwZdg"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
